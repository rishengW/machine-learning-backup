{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ffa401",
   "metadata": {},
   "source": [
    "# Assignment 2: Classification and Evaluation (20 marks)\n",
    "\n",
    "Student Name: Wang Risheng\n",
    "\n",
    "Student ID: 1053051\n",
    "\n",
    "## General info\n",
    "\n",
    "<b>Due date</b>: Monday, 1 September 2023 5pm\n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day up to 5 days (both weekdays and weekends count)\n",
    "<ul>\n",
    "    <li>one day late, -2.0;</li>\n",
    "    <li>two days late, -4.0;</li>\n",
    "    <li>three days late, -6.0;</li>\n",
    "    <li>four days late, -8.0;</li>\n",
    "    <li>five days late, -10.0;</li>\n",
    "</ul>\n",
    "\n",
    "<b>Marks</b>:  This assignment will be marked out of 20, and make up 20% of your overall mark for this subject.\n",
    "\n",
    "<b>Materials</b>: See [Using Jupyter Notebook and Python page] on Canvas (under Modules> Coding Resources) for information on the basic setup required for this class, including an iPython notebook viewer and the python packages `numpy`, `pandas`, `matplotlib` and `sklearn`. You can use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on Canvas>Assignments>Assignmnet2; we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: This assignment is an individual task, and so reuse of code or other instances of clear influence will be considered cheating. Please check the <a href=\"https://canvas.lms.unimelb.edu.au/courses/151131/modules#module_825112\">CIS Academic Honesty training</a> for more information. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place. Content produced by an AI (including, but not limited to ChatGPT) is not your own work, and submitting such content will be treated as a case of academic misconduct, in line with the <a href=\"https://academicintegrity.unimelb.edu.au/plagiarism-and-collusion/artificial-intelligence-tools-and-technologies\"> University's policy</a>.\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "Please carefully read and fill out the <b>Authorship Declaration</b> form at the bottom of the page. Failure to fill out this form results in the following deductions: \n",
    "<UL TYPE=”square”>\n",
    "<LI>Missing Authorship Declaration at the bottom of the page, -2.0\n",
    "<LI>Incomplete or unsigned Authorship Declaration at the bottom of the page, -1.0\n",
    "</UL>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d47f4",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "For this assignment, you will apply a number of classifiers to various datasets, and\n",
    "explore various evaluation paradigms and analyze the impact of multiple parameters on the performance of the classifiers. You will then answer a number of conceptual\n",
    "questions about the Naive Bayes classifier, K-nearest neighbors, and a number of baselines based on your observations. \n",
    "## Data Sets:\n",
    "In this assignment, you will work with two datasets. These datasets are adapted from a UCI archive public dataset:\n",
    "\n",
    " - **Adult**: You predict whether an adult person earns less than 50K or 50K or more US dollar per year, based on various personal attributes like age or education level. More information can be found<a href=\"https://archive.ics.uci.edu/dataset/2/adult\"> here </a>. \n",
    " - **Student**: You predict a student’s final grade {A+, A, B, C, D, F} based on a number of personal and performance related attributes, such as school, parent’s education level, number of absences, etc. More information can be found<a href=\"https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success\"> here </a>. \n",
    " \n",
    "More information about these datasets can be found in `readme.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b26164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa339cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB,CategoricalNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d90b111-3c9f-4b4c-a0a6-413000636fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# ignore future warnings \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a1c496",
   "metadata": {},
   "source": [
    "## Question 1. Reading and Pre-processing [1.5 marks] \n",
    "\n",
    "**A)** First, you will read in the data using the `fileName` parameter into a pandas DataFrame. You will also need to input the list of numerical feature names `num_feat` to the function to make your pre-processing easier.\n",
    "\n",
    "**B)** Second, you replace missing values denoted by `?` using the following two strategies: \n",
    "\n",
    "   * <b>Continuous features</b>: For each feature find the <b>average feature value</b> in the dataset \n",
    "   * <b>Categorical features</b>: For each feature find the <b>most frequent value</b> in the dataset  \n",
    "\n",
    "\n",
    "**C)** Third, you will use one-hot encoding to convert all nominal (and ordinal) attributes to numeric. You can achieve this by either using `get_dummies()` from the pandas library or `OneHotEncoder()` from the scikit-learn library. The resulting dataset includes all originally numeric features as well as the one-hot encoded features that are now numeric, call this data `num_dataset`.\n",
    "\n",
    "**D)** Fourth, you will use **equal-width** binning ( 4 bins ) to convert numerical features into categorical. You can achieve this by using `cut()` from pandas library. The resulting dataset includes all originally categorical features as well as the discretized features that are now categorical, call this data `cat_dataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c33a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should read a csv file and return two pandas dataframes\n",
    "\n",
    "def preprocess(fileName,num_feat):\n",
    "    ## read the csv file\n",
    "    data = pd.read_csv(fileName)\n",
    "    categorical_headers = []\n",
    "\n",
    "    ## replace missing values with the most frequent for categorical feature\n",
    "    for feature in data.columns[1:-1]:\n",
    "        if feature in num_feat:\n",
    "            data[feature] = pd.to_numeric(data[feature], errors='coerce')\n",
    "            mean = data[feature].mean()\n",
    "            data[feature].fillna(mean, inplace=True)\n",
    "    \n",
    "    ## replace missing values with the average for numerical features\n",
    "        else:\n",
    "            categorical_headers.append(feature)\n",
    "            mode_values = data[feature].mode()\n",
    "            mode_value = mode_values[0] # We usually pick the first value in case of there are multiple values with same frequency\n",
    "            data.loc[data[feature] == \"?\", feature] = mode_value                \n",
    "        \n",
    "    ## convert categorical features to numeric using one-hot encoding\n",
    "    \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "    # I mainly use the methods like fit_transform and get_features_names_out introduced in this website\n",
    "    dataset = data.copy()\n",
    "    encoder = OrdinalEncoder()\n",
    "    label_data = (dataset.iloc[:, -1]).values\n",
    "    label_data = label_data.reshape(-1,1)\n",
    "    encoded_label = pd.DataFrame(encoder.fit_transform(label_data))\n",
    "    encoded = pd.get_dummies(dataset[categorical_headers], drop_first=True)\n",
    "    num_dataset = pd.concat([dataset[num_feat], encoded, encoded_label], axis = 1)\n",
    "    \n",
    "    ## convert numerical features to categorical using equal-width binning\n",
    "    \n",
    "    cat_dataset = data.copy() # Since we need to reuse the original data\n",
    "    num_bins = 4\n",
    "    for feature in num_feat:\n",
    "        min_value = data[feature].min()\n",
    "        max_value = data[feature].max()\n",
    "        edges = np.linspace(min_value, max_value, num_bins + 1)\n",
    "        cat_dataset[feature] = pd.cut(data[feature], edges, labels=False, include_lowest=True)\n",
    "    cat_dataset.drop(columns = ['ID'])\n",
    "    return cat_dataset, num_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continental-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "## list of numeric features for adult dataset\n",
    "adult_num = ['Age','fnlwgt','Education-num','Capital-gain','Capital-loss','Hours-per-week']\n",
    "## generate the categorical and numerical adult datasets\n",
    "adult_cat_dataset,adult_num_dataset = preprocess(\"datasets/adult.csv\",adult_num)\n",
    "#print(adult_num_dataset.shape)\n",
    "#print(adult_cat_dataset.shape)\n",
    "adult_cat_dataset\n",
    "## generate the categorical and numerical student datasets\n",
    "student_cat_dataset,student_num_dataset = preprocess(\"datasets/student.csv\",[])\n",
    "#print(student_num_dataset.shape)\n",
    "#print(student_cat_dataset.shape)\n",
    "#student_num_dataset.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-joint",
   "metadata": {},
   "source": [
    "#### Question 2 . Baseline methods and Discussion [4.5 marks]\n",
    "**A)** For 10 rounds, use `train_test_split` to divide the processed `cat_dataset` into 80% train, 20% test . Set the `random_state` equal to the loop counter. For example in the loop\n",
    "``` python \n",
    "for i in range(10):\n",
    "```\n",
    "make `random_state` equal to `i`. \n",
    "Use the splitted datasets to train and test the following models: **[1 mark]**\n",
    "\n",
    "- Zero-R\n",
    "- One-R\n",
    "- Weighted Random \n",
    "\n",
    "Report the average accuracy over the 10 runs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "south-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can define your helper functions for One-R or other baselines in this block\n",
    "## for One-R at training time, you can break the ties randomly\n",
    "## for One-R at prediction time, if the test contains an unseen feature value, return the majority class\n",
    "def one_R_model(X_train, X_test, y_train, y_test):\n",
    "    feature_selected = None\n",
    "    optimal_accuracy = 0\n",
    "    final_dict = dict()\n",
    "\n",
    "    for attribute in X_train.columns[1:-1]:\n",
    "        predicts = []\n",
    "        attribute_counter = Counter(X_train[attribute])\n",
    "        class_counter = Counter()\n",
    "        class_dict = dict()\n",
    "        most_frequency = 0\n",
    "        total = 0\n",
    "        for value in attribute_counter:\n",
    "            class_labels = y_train[X_train[attribute] == value]\n",
    "            class_counter = Counter(class_labels)\n",
    "            total += class_counter.total()\n",
    "            most_common_one, frequency = class_counter.most_common(1)[0]\n",
    "            most_frequency += frequency\n",
    "            class_dict[value] = most_common_one\n",
    "            accuracy = most_frequency/total\n",
    "            \n",
    "        if accuracy > optimal_accuracy:\n",
    "            optimal_accuracy = accuracy\n",
    "            feature_selected = attribute\n",
    "            final_dict = class_dict\n",
    "        \n",
    "        success = 0\n",
    "        total = 0\n",
    "        counter_final = Counter(final_dict)\n",
    "        \n",
    "        for i in range(len(X_test[feature_selected])):\n",
    "            value = X_test[feature_selected].iloc[i]\n",
    "            \n",
    "            predict_value = None\n",
    "            if value not in final_dict:\n",
    "                most_one, freq = counter_final.most_common(1)[0]\n",
    "                predict_value = most_one\n",
    "            else:\n",
    "                predict_value = final_dict[value]\n",
    "            predicts.append(predict_value)\n",
    "\n",
    "            actual_label = X_test.iloc[i,-1]\n",
    "            if str(predict_value) in str(actual_label):\n",
    "                success += 1\n",
    "            total += 1\n",
    "        score = success / total\n",
    "    \n",
    "\n",
    "    return score, predicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e3d435-af7a-4bd4-b481-f22b1c7550a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Dataset Baseline results:\n",
      "Accuracy of ZeroR: 0.76\n",
      "Accuracy of One-R: 0.77\n",
      "Accuracy of Weighted Random: 0.64\n",
      "Student Dataset Baseline results:\n",
      "Accuracy of ZeroR: 0.3\n",
      "Accuracy of One-R: 0.31\n",
      "Accuracy of Weighted Random: 0.23\n"
     ]
    }
   ],
   "source": [
    "def baselines(cat_dataset):\n",
    "\n",
    "    ZeroR_Acc_1 = []\n",
    "    OneR_Acc_1 = []\n",
    "    WRand_Acc_1 = []\n",
    "\n",
    "    ## your code here\n",
    "    # First, we split them into 2 parts which are test part and train part respectively\n",
    "    runs = 10\n",
    "    test_portion = 0.2\n",
    "    X = cat_dataset\n",
    "    y = cat_dataset.iloc[:,-1]\n",
    "    predict_list = []\n",
    "    for i in range(runs):\n",
    "        counter = i\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = counter, test_size = test_portion\n",
    "        , shuffle = True)\n",
    "        \n",
    "        # Zero-R model\n",
    "        dummy_clf = DummyClassifier(strategy = \"most_frequent\")\n",
    "        dummy_clf.fit(X_train, y_train)\n",
    "        dummy_predict = dummy_clf.predict(X_test)\n",
    "        dummy_score = accuracy_score(y_test, dummy_predict)\n",
    "        ZeroR_Acc_1.append(dummy_score)\n",
    "        \n",
    "\n",
    "        # One-R model\n",
    "        score, predicts = one_R_model(X_train, X_test, y_train, y_test)\n",
    "        predict_list.append(predicts)\n",
    "        OneR_Acc_1.append(score)\n",
    "\n",
    "        # Weighted Random model      \n",
    "        WR_clf = DummyClassifier(strategy = \"stratified\")\n",
    "        WR_clf.fit(X_train, y_train)\n",
    "        WR_predictions = WR_clf.predict(X_test)\n",
    "        WR_acc = accuracy_score(y_test, WR_predictions)\n",
    "        WRand_Acc_1.append(WR_acc)\n",
    "        \n",
    "\n",
    "    \n",
    "    print(\"Accuracy of ZeroR:\", np.mean(ZeroR_Acc_1).round(2))\n",
    "    print(\"Accuracy of One-R:\", np.mean(OneR_Acc_1).round(2))\n",
    "    print(\"Accuracy of Weighted Random:\", np.mean(WRand_Acc_1).round(2))\n",
    "    \n",
    "##Adult Dataset and Student Dataset results: \n",
    "print(\"Adult Dataset Baseline results:\")\n",
    "baselines(adult_cat_dataset)\n",
    "\n",
    "print(\"Student Dataset Baseline results:\")\n",
    "baselines(student_cat_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c394c5",
   "metadata": {},
   "source": [
    "**B)** After comparing the performance of the different models on the classification task, please comment on any differences or lack of differences you observe between the baseline models and the datasets. **[1.5 marks]**</br>\n",
    "*NOTE: You may need to compare other performance metrics of these models, such as precision and recall of each class label, to gain a better understanding of their performance. You can use the `classification_report` from `sklearn.metrics` for this matter and check the performance of the classifiers for one round.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9f91c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-R model--adult data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      1.00      0.87       386\n",
      "         1.0       0.00      0.00      0.00       114\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.39      0.50      0.44       500\n",
      "weighted avg       0.60      0.77      0.67       500\n",
      "\n",
      "Zero-R model--student data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00        15\n",
      "          A+       0.00      0.00      0.00         5\n",
      "           B       0.00      0.00      0.00        29\n",
      "           C       0.00      0.00      0.00        26\n",
      "           D       0.29      1.00      0.45        38\n",
      "           F       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.29       130\n",
      "   macro avg       0.05      0.17      0.08       130\n",
      "weighted avg       0.09      0.29      0.13       130\n",
      "\n",
      "\n",
      "One-R model--adult data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.05      0.10       386\n",
      "         1.0       0.46      0.05      0.09       114\n",
      "    294708.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.05       500\n",
      "   macro avg       0.38      0.03      0.06       500\n",
      "weighted avg       0.62      0.05      0.10       500\n",
      "\n",
      "One-R model--student data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00        15\n",
      "          A+       0.00      0.00      0.00         5\n",
      "           B       0.00      0.00      0.00        29\n",
      "           C       0.35      0.31      0.33        26\n",
      "           D       0.31      0.87      0.46        38\n",
      "           F       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.32       130\n",
      "   macro avg       0.11      0.20      0.13       130\n",
      "weighted avg       0.16      0.32      0.20       130\n",
      "\n",
      "\n",
      "Weighted Random model--adult data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.78      0.78       386\n",
      "         1.0       0.24      0.24      0.24       114\n",
      "\n",
      "    accuracy                           0.66       500\n",
      "   macro avg       0.51      0.51      0.51       500\n",
      "weighted avg       0.65      0.66      0.65       500\n",
      "\n",
      "Weighted Random model--student data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.22      0.13      0.17        15\n",
      "          A+       0.00      0.00      0.00         5\n",
      "           B       0.00      0.00      0.00        29\n",
      "           C       0.14      0.19      0.16        26\n",
      "           D       0.15      0.16      0.15        38\n",
      "           F       0.04      0.06      0.05        17\n",
      "\n",
      "    accuracy                           0.11       130\n",
      "   macro avg       0.09      0.09      0.09       130\n",
      "weighted avg       0.10      0.11      0.10       130\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = student_cat_dataset\n",
    "y = student_cat_dataset.iloc[:,-1]\n",
    "\n",
    "x = adult_num_dataset\n",
    "Y = adult_num_dataset.iloc[:, -1]\n",
    "random_state_num = 6\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = random_state_num, test_size = 0.2 )\n",
    "new_x_train, new_x_test, new_y_train, new_y_test = train_test_split(x, Y, random_state = random_state_num, test_size = 0.2)\n",
    "print(\"Zero-R model--adult data\")\n",
    "dummy_clf_adult = DummyClassifier(strategy = \"most_frequent\")\n",
    "dummy_clf_adult.fit(new_x_train, new_y_train)\n",
    "dummy_predict_adult= dummy_clf_adult.predict(new_x_test)\n",
    "print(classification_report(new_y_test,dummy_predict_adult, zero_division = 0))\n",
    "print(\"Zero-R model--student data\")\n",
    "dummy_clf = DummyClassifier(strategy = \"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_predict = dummy_clf.predict(X_test)\n",
    "dummy_score = accuracy_score(y_test, dummy_predict)\n",
    "print(classification_report(y_test,dummy_predict, zero_division = 0))\n",
    "print()\n",
    "print(\"One-R model--adult data\")\n",
    "score_adult, predicts_adult = one_R_model(new_x_train, new_x_test, new_y_train, new_y_test)\n",
    "print(classification_report(new_y_test, predicts_adult, zero_division = 0))\n",
    "print(\"One-R model--student data\")\n",
    "score, predicts = one_R_model(X_train, X_test, y_train, y_test)\n",
    "print(classification_report(y_test, predicts, zero_division = 0))\n",
    "print()\n",
    "\n",
    "print(\"Weighted Random model--adult data\")\n",
    "WR_clf_adult = DummyClassifier(strategy = \"stratified\")\n",
    "WR_clf_adult.fit(new_x_train, new_y_train)\n",
    "WR_predictions_adult = WR_clf_adult.predict(new_x_test)\n",
    "print(classification_report(new_y_test,WR_predictions_adult, zero_division = 0))\n",
    "print(\"Weighted Random model--student data\")\n",
    "WR_clf = DummyClassifier(strategy = \"stratified\")\n",
    "WR_clf.fit(X_train, y_train)\n",
    "WR_predictions = WR_clf.predict(X_test)\n",
    "WR_acc = accuracy_score(y_test, WR_predictions)\n",
    "print(classification_report(y_test,WR_predictions, zero_division = 0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3936c2-b548-4825-8028-c5b3f3800b63",
   "metadata": {},
   "source": [
    "*Answer Here*  \n",
    "\n",
    "\n",
    "Essentially, I use the categorical dataset generated from student csv file since there are multiple labels and is representative. \n",
    "\n",
    "Based on the definition of recall and precision. precision is computed by the portion of true positive in the sum of positive results while recall is defined as the portion of true positive in the sum of true positive and false negative, and they are in reverse relationship. \n",
    "\n",
    "In zero-R model, the precision value of each label is much smaller than recall value of each label. It means that the zero-R model cannot predict the labels correctly in most cases. For class D, the recall value is 1, which indicates that all the labels are identified as D in majority of instances but not others. Therefore, the f1 value, defined as 2 * precision * recall / (precision + recall), is highly affected and is 0 in most classes.\n",
    "\n",
    "In one-R model, the performance of the model is still not well. Even though there is one more class has the non-zero precision value but 2 thirds of classes have 0 precision values. This means that predictions are correct in limit number of classes. Recall values reflect that the model mainly uses the instances, which are labelled as D, to do the prediction. Thereby, the values of f1 score are 0 in most classes.\n",
    "\n",
    "In weighted random model, the range of precision values is roughly big, meaning that the correctness of labelling prediction varies across the classes. For instance, the model prefer to label the instance as C to label it as A+. Due to the relationship between precision and recall, recall values hava a large range. As a result, the values of f1 score also vary across the classes. the performance of this model is still not good.\n",
    "\n",
    "As discussed above, these 3 models do not have strong performance because they mainly use small amounts of classes to predict classes in majority of instances\n",
    "and use only limited aspects to predict the classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-worker",
   "metadata": {},
   "source": [
    "**C)** Update your code for One-R so that you can inspect the feature that is most often selected in the 10 rounds of training and testing for each dataset. Write the classification rule using the best feature and its values for each dataset. **[1 mark]**</br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2260f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medu\n",
      "Fedu\n",
      "Fedu\n",
      "Medu\n",
      "Medu\n",
      "Medu\n",
      "Fedu\n",
      "Fedu\n",
      "Fedu\n",
      "Fedu\n",
      "Accuracy of One-R: 0.3\n"
     ]
    }
   ],
   "source": [
    "OneR_Acc_1 = []\n",
    "runs = 10\n",
    "for i in range(runs):\n",
    "        counter = i\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = counter, test_size = 0.2\n",
    "        , shuffle = True)\n",
    "        \n",
    "        ## You can define your helper functions for One-R or other baselines in this block\n",
    "        ## for One-R at training time, you can break the ties randomly\n",
    "        ## for One-R at prediction time, if the test contains an unseen feature value, return the majority class\n",
    "        feature_selected = None\n",
    "        optimal_accuracy = 0\n",
    "        final_dict = dict()\n",
    "        overall = Counter()\n",
    "        \n",
    "        for attribute in X_train.columns[1:-1]:\n",
    "                predicts = []\n",
    "                attribute_counter = Counter(X_train[attribute])\n",
    "                class_counter = Counter()\n",
    "                class_dict = dict()\n",
    "                most_frequency = 0\n",
    "                total = 0\n",
    "                for value in attribute_counter:\n",
    "                        class_labels = y_train[X_train[attribute] == value]\n",
    "                        class_counter = Counter(class_labels)\n",
    "                        total += class_counter.total()\n",
    "                        most_common_one, frequency = class_counter.most_common(1)[0]\n",
    "                        most_frequency += frequency\n",
    "                        class_dict[value] = most_common_one\n",
    "                        accuracy = most_frequency/total\n",
    "                \n",
    "                if accuracy > optimal_accuracy:\n",
    "                        optimal_accuracy = accuracy\n",
    "                        feature_selected = attribute\n",
    "                        final_dict = class_dict\n",
    "                \n",
    "                success = 0\n",
    "                total = 0\n",
    "                counter_final = Counter(final_dict)\n",
    "                \n",
    "                for i in range(len(X_test[feature_selected])):\n",
    "                        value = X_test[feature_selected].iloc[i]\n",
    "                \n",
    "                        predict_value = None\n",
    "                        if value not in final_dict:\n",
    "                                most_one, freq = counter_final.most_common(1)[0]\n",
    "                                predict_value = most_one\n",
    "                        \n",
    "                        else:\n",
    "                                predict_value = final_dict[value]\n",
    "                        predicts.append(predict_value)\n",
    "\n",
    "                        actual_label = X_test.iloc[i][-1]\n",
    "                        \n",
    "                        if str(predict_value) in str(actual_label):\n",
    "                                success += 1\n",
    "                        total += 1\n",
    "                score = success / total\n",
    "                OneR_Acc_1.append(score)\n",
    "        print(feature_selected)\n",
    "print(\"Accuracy of One-R:\", np.mean(OneR_Acc_1).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-snowboard",
   "metadata": {},
   "source": [
    "*Answer Here*  \n",
    "The feature selected is basically decided across the all possible values in each feature. In each feature, I will sum up the number of class labels which are labelled in most insatnces for each value in this feature and then divide it by the total number of instances to get the ratio. The feature with greatese ratio will be selected to do further process. I use the class labelled in most instances in a value in the feature selected to predict the test data set, for example, if majority of instances, which are labelled A, and the almost values in feature selected are 1, then when I come to the test data set, all the instances with value 1 in feature selected will be labelled A.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-tiffany",
   "metadata": {},
   "source": [
    "**D)** For weighted random baseline applied to Adult dataset, what would the error rate converge to (Write a formula based on the prior probability of the dominant class, named `prior`, and the fraction of test samples belonging to the dominant class, `fraction`)? **[1 mark]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-convention",
   "metadata": {},
   "source": [
    "*Answer Here*\n",
    "\n",
    "Essentially, Weighted random models randomly assigns the label probablities using train data set and then use the probability to do prediction. Error rate is computed as wrong predictions divided by total number of predictions.\n",
    "\n",
    "\n",
    "=> prior probability of dominant class in test dataset is N * prior\n",
    "   actual fraction of test samples belonging to the dominant class is N * fraction, where N is the total number of perdicitions\n",
    "\n",
    "=>error rate is the absolute value of difference between N * prior and N * fraction, and then divided by N ,also written as abs(N * prior - N * fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec6a7b",
   "metadata": {},
   "source": [
    "## Question 3. Naive Bayes models [5 marks]\n",
    "\n",
    "**A)** Divide the `num_dataset` and `cat_dataset` into 80% train and 20% test splits for 10 rounds, set the `random_state` equal to the loop counter. Then, train and test the following models:\n",
    "\n",
    "- Gaussian Naive Bayes\n",
    "- Bernoulli Naive Bayes\n",
    "- Categorical Naive Bayes \n",
    "\n",
    "You must use the input data that you believe is best suited for each model. Finally, report the average accuracy of the NB models over the 10 runs. **[1 mark]**\n",
    "\n",
    "**Note: You may need to change your input format to be able to use sklearn's CategoricalNB.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ef637cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Dataset NB results:\n",
      "Accuracy of GNB: 0.8\n",
      "Accuracy of BNB: 0.79\n",
      "Accuracy of CNB: 0.76\n",
      "Student Dataset NB results:\n",
      "Accuracy of GNB: 0.17\n",
      "Accuracy of BNB: 0.31\n",
      "Accuracy of CNB: 0.3\n"
     ]
    }
   ],
   "source": [
    "def NB_models(num_dataset,cat_dataset):\n",
    "\n",
    "    GNB_Acc_1 = []\n",
    "    BNB_Acc_1 = []\n",
    "    CNB_Acc_1 = []\n",
    "\n",
    "    ## your code here\n",
    "    test_portion = 0.2\n",
    "    runs = 10\n",
    "    \n",
    "    for i in range(runs):\n",
    "        counter = i\n",
    "        X_train, X_test, y_train, y_test = train_test_split(num_dataset.iloc[:,:-1], num_dataset.iloc[:, -1], train_size = 1 - test_portion, test_size = test_portion, random_state = counter)\n",
    "        \n",
    "        ## Gaussian Naive Bayes\n",
    "        gaussian_clf = GaussianNB()\n",
    "        gaussian_clf.fit(X_train, y_train)\n",
    "        gaussian_predictions = gaussian_clf.predict(X_test)\n",
    "        gaussian_acc = accuracy_score(y_test, gaussian_predictions)\n",
    "        GNB_Acc_1.append(gaussian_acc)\n",
    "\n",
    "        ## Bernoulli Naive Bayes\n",
    "        bernoulli_clf = BernoulliNB()\n",
    "        bernoulli_clf.fit(X_train, y_train)\n",
    "        bernoulli_predictions = bernoulli_clf.predict(X_test)\n",
    "        bernoulli_acc = accuracy_score(y_test, bernoulli_predictions)\n",
    "        BNB_Acc_1.append(bernoulli_acc)\n",
    "\n",
    "        ## Categorical Naive Bayes\n",
    "        encoder = OrdinalEncoder()\n",
    "        labels = cat_dataset.iloc[:,-1]\n",
    "        labels_2d = labels.values.reshape(-1, 1)\n",
    "        labels_encode = encoder.fit_transform(labels_2d)\n",
    "        \n",
    "        new_dataset = cat_dataset\n",
    "        header = []\n",
    "        for feature in new_dataset.columns[:-1]:\n",
    "            header.append(feature)\n",
    "        encoded = pd.get_dummies(new_dataset[header], drop_first=True)\n",
    "        \n",
    "        train, test, label_train, label_test= train_test_split(encoded, labels_encode, test_size = test_portion, random_state = counter)\n",
    "        # https://github.com/scikit-learn/scikit-learn/issues/16028\n",
    "        # In this link, I foound out the way to fix the error by typing different parameters in CNB\n",
    "        categorical_clf = CategoricalNB(min_categories=7)\n",
    "        new_train_label = label_train.ravel()\n",
    "        new_test_label = label_test.ravel()\n",
    "        categorical_clf.fit(train, new_train_label)\n",
    "        categorical_predictions = categorical_clf.predict(test)\n",
    "        categorical_score = accuracy_score(categorical_predictions,new_test_label)\n",
    "        CNB_Acc_1.append(categorical_score)\n",
    "\n",
    "    \n",
    "    print(\"Accuracy of GNB:\", np.mean(GNB_Acc_1).round(2))\n",
    "    print(\"Accuracy of BNB:\", np.mean(BNB_Acc_1).round(2))            \n",
    "    print(\"Accuracy of CNB:\", np.mean(CNB_Acc_1).round(2))\n",
    "    \n",
    "    \n",
    "\n",
    "##Adult Dataset and Student Dataset results: \n",
    "print(\"Adult Dataset NB results:\")\n",
    "NB_models(adult_num_dataset,adult_cat_dataset)\n",
    "\n",
    "\n",
    "print(\"Student Dataset NB results:\")\n",
    "NB_models(student_num_dataset,student_cat_dataset)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-berry",
   "metadata": {},
   "source": [
    "**B)** How does the performance of the Naive Bayes classifiers compare against your baseline models for each dataset? **[1 mark]** Please comment on any differences you observe between the baseline models and the NB models in the context of the two datasets.</br> *NOTE: You may need to compare other performance metrics of these models, such as precision and recall of each class label, to gain a better understanding of their performance. You can use the `classification_report` from `sklearn.metrics` for this matter and check the performance of the classifiers for one round.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc6576eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------adult_GNB-------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.94      0.87       386\n",
      "         1.0       0.58      0.29      0.39       114\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.70      0.61      0.63       500\n",
      "weighted avg       0.76      0.79      0.76       500\n",
      "\n",
      "----------------------student_GNB-------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.14      0.40      0.21        15\n",
      "         1.0       0.02      0.20      0.04         5\n",
      "         2.0       0.17      0.10      0.13        29\n",
      "         3.0       0.00      0.00      0.00        26\n",
      "         4.0       0.67      0.05      0.10        38\n",
      "         5.0       0.43      0.35      0.39        17\n",
      "\n",
      "    accuracy                           0.14       130\n",
      "   macro avg       0.24      0.18      0.14       130\n",
      "weighted avg       0.30      0.14      0.13       130\n",
      "\n",
      "----------------------adult_BNB-------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.96      0.88       386\n",
      "         1.0       0.63      0.24      0.34       114\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.72      0.60      0.61       500\n",
      "weighted avg       0.77      0.79      0.76       500\n",
      "\n",
      "----------------------student_BNB-------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.13      0.20      0.16        15\n",
      "         1.0       0.00      0.00      0.00         5\n",
      "         2.0       0.27      0.21      0.24        29\n",
      "         3.0       0.21      0.35      0.26        26\n",
      "         4.0       0.46      0.34      0.39        38\n",
      "         5.0       0.46      0.35      0.40        17\n",
      "\n",
      "    accuracy                           0.28       130\n",
      "   macro avg       0.26      0.24      0.24       130\n",
      "weighted avg       0.31      0.28      0.29       130\n",
      "\n",
      "----------------------adult_CNB-------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      1.00      0.87       386\n",
      "         1.0       0.00      0.00      0.00       114\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.39      0.50      0.44       500\n",
      "weighted avg       0.60      0.77      0.67       500\n",
      "\n",
      "----------------------student_CNB-------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        15\n",
      "         1.0       0.00      0.00      0.00         5\n",
      "         2.0       0.00      0.00      0.00        29\n",
      "         3.0       0.00      0.00      0.00        26\n",
      "         4.0       0.29      1.00      0.45        38\n",
      "         5.0       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.29       130\n",
      "   macro avg       0.05      0.17      0.08       130\n",
      "weighted avg       0.09      0.29      0.13       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_portion = 0.2\n",
    "num_dataset = student_num_dataset\n",
    "cat_dataset = student_cat_dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(num_dataset.iloc[:,:-1], num_dataset.iloc[:, -1], train_size = 1 - test_portion, test_size = test_portion, random_state = 6)\n",
    "new_x_train, new_x_test, new_y_train, new_y_test = train_test_split(adult_num_dataset.iloc[:,:-1], adult_num_dataset.iloc[:,-1], train_size = 1 - test_portion, test_size = test_portion, random_state = 6)       \n",
    "\n",
    "## Gaussian Naive Bayes\n",
    "gaussian_clf_adult = GaussianNB()\n",
    "gaussian_clf_adult.fit(new_x_train, new_y_train)\n",
    "gaussian_predictions_adult = gaussian_clf_adult.predict(new_x_test)\n",
    "print(\"----------------------adult_GNB-------------------------\")\n",
    "print(classification_report(new_y_test, gaussian_predictions_adult, zero_division = 0))\n",
    "gaussian_clf_student = GaussianNB()\n",
    "gaussian_clf_student.fit(X_train, y_train)\n",
    "gaussian_predictions_student = gaussian_clf_student.predict(X_test)\n",
    "print(\"----------------------student_GNB-------------------------\")\n",
    "print(classification_report(y_test, gaussian_predictions_student, zero_division = 0))\n",
    "\n",
    "## Bernoulli Naive Bayes\n",
    "bernoulli_clf_adult = BernoulliNB()\n",
    "bernoulli_clf_adult.fit(new_x_train, new_y_train)\n",
    "bernoulli_predictions_adult = bernoulli_clf_adult.predict(new_x_test)\n",
    "print(\"----------------------adult_BNB-------------------------\")\n",
    "print(classification_report(new_y_test, bernoulli_predictions_adult, zero_division = 0))\n",
    "bernoulli_clf_student = BernoulliNB()\n",
    "bernoulli_clf_student.fit(X_train, y_train)\n",
    "bernoulli_predictions_student = bernoulli_clf_student.predict(X_test)\n",
    "print(\"----------------------student_BNB-------------------------\")\n",
    "print(classification_report(y_test, bernoulli_predictions_student, zero_division = 0))\n",
    "\n",
    "## Categorical Naive Bayes\n",
    "encoder = OrdinalEncoder()\n",
    "labels = cat_dataset.iloc[:,-1]\n",
    "labels_2d = labels.values.reshape(-1, 1)\n",
    "labels_encode = encoder.fit_transform(labels_2d) \n",
    "new_dataset = cat_dataset    \n",
    "header = []\n",
    "for feature in new_dataset.columns[:-1]:\n",
    "    header.append(feature)\n",
    "encoded = pd.get_dummies(new_dataset[header], drop_first=True)\n",
    "\n",
    "encoder_2 = OrdinalEncoder()\n",
    "labels_2 = adult_cat_dataset.iloc[:,-1]\n",
    "labels_2d_2 = labels_2.values.reshape(-1, 1)\n",
    "labels_encode_2 = encoder_2.fit_transform(labels_2d_2) \n",
    "new_dataset_2 = adult_cat_dataset    \n",
    "header_2 = []\n",
    "for feature in new_dataset_2.columns[:-1]:\n",
    "    header_2.append(feature)\n",
    "encoded_2 = pd.get_dummies(new_dataset_2[header_2], drop_first=True)\n",
    "        \n",
    "train, test, label_train, label_test= train_test_split(encoded, labels_encode, test_size = test_portion, random_state = 6)\n",
    "new_train, new_test, new_label_train, new_label_test = train_test_split(encoded_2, labels_encode_2,test_size = test_portion, random_state = 6)\n",
    "categorical_clf_2 = CategoricalNB(min_categories=7)\n",
    "new_train_label_2 = new_label_train.ravel()\n",
    "new_test_label_2 = new_label_test.ravel()\n",
    "categorical_clf_2.fit(new_train, new_train_label_2)\n",
    "categorical_predictions_2 = categorical_clf_2.predict(new_test)\n",
    "print(\"----------------------adult_CNB-------------------------\")\n",
    "print(classification_report(new_label_test, categorical_predictions_2, zero_division = 0))\n",
    "categorical_clf = CategoricalNB(min_categories=7)\n",
    "new_train_label = label_train.ravel()\n",
    "new_test_label = label_test.ravel()\n",
    "categorical_clf.fit(train, new_train_label)\n",
    "categorical_predictions = categorical_clf.predict(test)\n",
    "print(\"----------------------student_CNB-------------------------\")\n",
    "print(classification_report(label_test, categorical_predictions, zero_division = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-criterion",
   "metadata": {},
   "source": [
    "*Answer Here*  \n",
    "\n",
    "Accuracy:\n",
    "When we use adult data set, we can observe that Naive Bayes models can generate higher accuracy compared to the baseline models, meaning that NB classifiers can find the more complicated relationship between class and features in the file and it is able to find the dependencies between features.\n",
    "\n",
    "Now turns to student data set. NB models have worse performance this time and they give much lower accuracy which is close to that given by baseline methods. Both types of models do not work well. Complicated structure of dataset might be the reason that they estimate the low accuracy.\n",
    "\n",
    "Precision, recall and F1 score:\n",
    "In term of adult dataset, NB models generally outperform the baseline models by compairing  precision, recall and f1-score. Higher precision values and higher recall values indicate that NB can be better at distinguish the class. Higher f1 score means that NB can achieve better balance between precision and recall based on the formula of the f1 score.\n",
    "\n",
    "Now the student dataset. Baseline models outperform the NB models this time.\n",
    "Baseline models can identify the class for each instance in a better performance compared to NB models. The complexity of the relationships between features might be the reason why NB models have poor performance. But overall, both types of models do not perform well.\n",
    "\n",
    "In general, NB can have better performance than baseline methods based on either accuracy or (precision, recall, f1 score) evaluation when the dataset is not complicated. Both types of model may have poor performance if they use a complicated dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-information",
   "metadata": {},
   "source": [
    "**C)** The three Naive Bayes (NB) classifiers lead to different performances. Which of these NB classifiers performs best for each dataset, and why do you think it is the case? **[1 mark]** *NOTE: You may need to compare other performance metrics of these models, such as precision and recall of each class label, to gain a better understanding of their performance. You can use the `classification_report` from `sklearn.metrics` for this matter and check the performance of the classifiers for one round.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-collar",
   "metadata": {},
   "source": [
    "*Answer Here*\n",
    "\n",
    "Adult dataset:\n",
    "\n",
    "Bernoulli performs best. It can give high precision, high recall, high f1 scores and high accuracy.\n",
    "\n",
    "Student dataset:\n",
    "\n",
    "Bernoulli Naive Bayes is still the best one among them since it generates high precision, high recall, high f1 scores and high accuracy\n",
    "\n",
    "Naive Bayes models rely on how assumptions are deeply aligned with dataset. Bernoulli classifiers can tackle the imbalance in dataset by adjusting class priors. But it can only handle the dataset which is fully numeric.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-check",
   "metadata": {},
   "source": [
    "**D)** The Gaussian Naive Bayes classifier makes two fundamental assumptions: (1) about the distribution of $P(x_j|c_i)$ and (2) about the (conditional) dependency structure between features.\n",
    "Explain both assumptions, and discuss whether these assumptions are always true for the\n",
    "numeric attributes in the Adult dataset. If applicable, identify some cases where the assumptions are violated. **[2 marks]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-luther",
   "metadata": {},
   "source": [
    "*Answer Here*  \n",
    "\n",
    "In adult dataset, attributes like hours work per week and age could be distributed as Gaussian distribution. But attributes like capital loss may not be Gaussian distributed since they may be sparse and many of values are 0. Gaussian distribution indicates that values are symmetrically distribuited around mean value. Thus the values in capital loss may break the assumption.\n",
    "In adult dataset, education background may be correlated to the occupations, which breaks the independency between features, in other words, the independencies between variables. In this case, the second assumption may be violated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de4822d",
   "metadata": {},
   "source": [
    "## Question 4. K-Nearest Neighbor [3 marks] \n",
    "**A)** Divide the `num_dataset` into 80% train and 20% test splits for 10 rounds, set the `random_state` equal to the loop counter. Then, train and test the following models:\n",
    "\n",
    "- 6 K-Nearest Neighbor models with Euclidean distance using the following parameters:\n",
    "\n",
    "    - with K values of 1,5, and 10\n",
    "    \n",
    "    - using inverse distance weighting and majority voting \n",
    "\n",
    "Finally, report the average accuracy of the KNN models over the 10 rounds. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0859a139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Dataset KNN results:\n",
      "Accuracy of weighted KNN(1): 0.7\n",
      "Accuracy of weighted KNN(5): 0.74\n",
      "Accuracy of weighted KNN(10): 0.76\n",
      "Accuracy of KNN(1): 0.7\n",
      "Accuracy of KNN(5): 0.78\n",
      "Accuracy of KNN(10): 0.79\n",
      "Student Dataset KNN results:\n",
      "Accuracy of weighted KNN(1): 0.62\n",
      "Accuracy of weighted KNN(5): 0.72\n",
      "Accuracy of weighted KNN(10): 0.75\n",
      "Accuracy of KNN(1): 0.62\n",
      "Accuracy of KNN(5): 0.72\n",
      "Accuracy of KNN(10): 0.72\n"
     ]
    }
   ],
   "source": [
    "def KNNs(num_dataset):\n",
    "    KNN1_Acc_1_weighted = []\n",
    "    KNN5_Acc_1_weighted = []\n",
    "    KNN10_Acc_1_weighted = []\n",
    "    KNN1_Acc_1_majority = []\n",
    "    KNN5_Acc_1_majority = []\n",
    "    KNN10_Acc_1_majority = []\n",
    "\n",
    "\n",
    "    ## your code here\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "    # I mainly use the materials shown in this website\n",
    "    runs = 10\n",
    "    test_portion = 0.2\n",
    "    majority_predictions_list = []\n",
    "\n",
    "    for i in range(runs):\n",
    "        random_state = i\n",
    "        X_train, X_test, y_train, y_test = train_test_split(num_dataset, num_dataset.iloc[:,-1]\n",
    "        , test_size = test_portion, random_state = random_state)\n",
    "        \n",
    "        KNN_inverse_1 = KNeighborsClassifier(n_neighbors = 1, weights = 'distance')\n",
    "        KNN_inverse_5 = KNeighborsClassifier(n_neighbors = 5, weights = 'distance')\n",
    "        KNN_inverse_10 = KNeighborsClassifier(n_neighbors = 10, weights = 'distance')\n",
    "\n",
    "        KNN_majority_1 = KNeighborsClassifier(n_neighbors = 1, weights = 'uniform')\n",
    "        KNN_majority_5 = KNeighborsClassifier(n_neighbors = 5, weights = 'uniform')\n",
    "        KNN_majority_10 = KNeighborsClassifier(n_neighbors = 10, weights = 'uniform')\n",
    "\n",
    "        # Fit the model and calculate corresponding accuracy based on predicitions\n",
    "        #print(X_train.shape)\n",
    "        #print(y_train.shape)\n",
    "        #print(X_test.shape)\n",
    "        #print(y_test.shape)\n",
    "        KNN_inverse_1.fit(X_train.values, y_train)\n",
    "        KNN_inverse_5.fit(X_train.values, y_train)\n",
    "        KNN_inverse_10.fit(X_train.values, y_train)\n",
    "        KNN_majority_1.fit(X_train.values, y_train)\n",
    "        KNN_majority_5.fit(X_train.values, y_train)\n",
    "        KNN_majority_10.fit(X_train.values, y_train)\n",
    "        \n",
    "        inverse_predictions_1 = KNN_inverse_1.predict(X_test.values)\n",
    "        inverse_predictions_5 = KNN_inverse_5.predict(X_test.values)\n",
    "        inverse_predictions_10 = KNN_inverse_10.predict(X_test.values)\n",
    "        majority_predictions_1 = KNN_majority_1.predict(X_test.values)\n",
    "        majority_predictions_5 = KNN_majority_5.predict(X_test.values)\n",
    "        majority_predictions_10 = KNN_majority_10.predict(X_test.values)\n",
    "        \n",
    "        accuracy_inverse_1 = accuracy_score(y_test, inverse_predictions_1)\n",
    "        accuracy_inverse_5 = accuracy_score(y_test, inverse_predictions_5)\n",
    "        accuracy_inverse_10 = accuracy_score(y_test, inverse_predictions_10)\n",
    "        accuracy_majority_1 = accuracy_score(y_test, majority_predictions_1)\n",
    "        accuracy_majority_5 = accuracy_score(y_test, majority_predictions_5)\n",
    "        accuracy_majority_10 = accuracy_score(y_test, majority_predictions_10)\n",
    "\n",
    "        # Add the accuracies\n",
    "            \n",
    "        KNN1_Acc_1_weighted.append(accuracy_inverse_1)\n",
    "        KNN1_Acc_1_majority.append(accuracy_majority_1)\n",
    "            \n",
    "        KNN5_Acc_1_weighted.append(accuracy_inverse_5)\n",
    "        KNN5_Acc_1_majority.append(accuracy_majority_5)\n",
    "            \n",
    "        KNN10_Acc_1_weighted.append(accuracy_inverse_10)\n",
    "        KNN10_Acc_1_majority.append(accuracy_majority_10)\n",
    "        \n",
    "        k_NUM = 3\n",
    "        KNN_3_major = KNeighborsClassifier(n_neighbors = k_NUM)\n",
    "        KNN_3_major.fit(X_train.values, y_train)\n",
    "        KNN_3_predictions = KNN_3_major.predict(X_test.values)\n",
    "        majority_predictions_list.append(KNN_3_predictions)\n",
    "\n",
    "            \n",
    "    print(\"Accuracy of weighted KNN(1):\", np.mean(KNN1_Acc_1_weighted).round(2))\n",
    "    print(\"Accuracy of weighted KNN(5):\", np.mean(KNN5_Acc_1_weighted).round(2))\n",
    "    print(\"Accuracy of weighted KNN(10):\", np.mean(KNN10_Acc_1_weighted).round(2))\n",
    "    print(\"Accuracy of KNN(1):\", np.mean(KNN1_Acc_1_majority).round(2))\n",
    "    print(\"Accuracy of KNN(5):\", np.mean(KNN5_Acc_1_majority).round(2))\n",
    "    print(\"Accuracy of KNN(10):\", np.mean(KNN10_Acc_1_majority).round(2))\n",
    "    \n",
    "    #return majority_predictions_list\n",
    "    \n",
    "##Adult Dataset and Student Dataset results: \n",
    "\n",
    "print(\"Adult Dataset KNN results:\")\n",
    "KNNs(adult_num_dataset)\n",
    "\n",
    "print(\"Student Dataset KNN results:\")\n",
    "KNNs(student_num_dataset)\n",
    "\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8357cd59",
   "metadata": {},
   "source": [
    "**B)** Compare the results of the weighted and majority KNN models (for each value of K) and explain any differences you observe for each dataset in terms of the voting strategy and the number of nearest neighbors. **[1 marks]**</br> *NOTE: You may need to compare other performance metrics of these models, such as precision and recall of each class label, to gain a better understanding of their performance. You can use the `classification_report` from `sklearn.metrics` for this matter and check the performance of the classifiers for one round.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cba227aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------KNN_1_inverse-------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.82       386\n",
      "         1.0       0.36      0.34      0.35       114\n",
      "\n",
      "    accuracy                           0.71       500\n",
      "   macro avg       0.59      0.58      0.58       500\n",
      "weighted avg       0.71      0.71      0.71       500\n",
      "\n",
      "----------------------KNN_5_inverse-------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.89      0.84       386\n",
      "         1.0       0.41      0.26      0.32       114\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.61      0.58      0.58       500\n",
      "weighted avg       0.71      0.75      0.72       500\n",
      "\n",
      "---------------------KNN_10_inverse-------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.92      0.86       386\n",
      "         1.0       0.47      0.24      0.32       114\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.64      0.58      0.59       500\n",
      "weighted avg       0.73      0.77      0.74       500\n",
      "\n",
      "---------------------KNN_1_majority-------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.82       386\n",
      "         1.0       0.36      0.34      0.35       114\n",
      "\n",
      "    accuracy                           0.71       500\n",
      "   macro avg       0.59      0.58      0.58       500\n",
      "weighted avg       0.71      0.71      0.71       500\n",
      "\n",
      "---------------------KNN_5_majority-------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.95      0.87       386\n",
      "         1.0       0.55      0.19      0.29       114\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.68      0.57      0.58       500\n",
      "weighted avg       0.74      0.78      0.74       500\n",
      "\n",
      "---------------------KNN_10_majority-------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.99      0.88       386\n",
      "         1.0       0.75      0.13      0.22       114\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.77      0.56      0.55       500\n",
      "weighted avg       0.78      0.79      0.73       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_dataset = adult_num_dataset\n",
    "test_portion = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(num_dataset, num_dataset.iloc[:,-1]\n",
    "        , test_size = test_portion, random_state = 6)\n",
    "        \n",
    "KNN_inverse_1 = KNeighborsClassifier(n_neighbors = 1, weights = 'distance')\n",
    "KNN_inverse_5 = KNeighborsClassifier(n_neighbors = 5, weights = 'distance')\n",
    "KNN_inverse_10 = KNeighborsClassifier(n_neighbors = 10, weights = 'distance')\n",
    "\n",
    "KNN_majority_1 = KNeighborsClassifier(n_neighbors = 1, weights = 'uniform')\n",
    "KNN_majority_5 = KNeighborsClassifier(n_neighbors = 5, weights = 'uniform')\n",
    "KNN_majority_10 = KNeighborsClassifier(n_neighbors = 10, weights = 'uniform')\n",
    "\n",
    "# Fit the model and calculate corresponding precision based on predicitions\n",
    "\n",
    "KNN_inverse_1.fit(X_train.values, y_train)\n",
    "KNN_inverse_5.fit(X_train.values, y_train)\n",
    "KNN_inverse_10.fit(X_train.values, y_train)\n",
    "KNN_majority_1.fit(X_train.values, y_train)\n",
    "KNN_majority_5.fit(X_train.values, y_train)\n",
    "KNN_majority_10.fit(X_train.values, y_train)\n",
    "        \n",
    "inverse_predictions_1 = KNN_inverse_1.predict(X_test.values)\n",
    "inverse_predictions_5 = KNN_inverse_5.predict(X_test.values)\n",
    "inverse_predictions_10 = KNN_inverse_10.predict(X_test.values)\n",
    "majority_predictions_1 = KNN_majority_1.predict(X_test.values)\n",
    "majority_predictions_5 = KNN_majority_5.predict(X_test.values)\n",
    "majority_predictions_10 = KNN_majority_10.predict(X_test.values)\n",
    "print(\"----------------------KNN_1_inverse-------------------------\")\n",
    "print(classification_report(y_test, inverse_predictions_1, zero_division = 0))\n",
    "print(\"----------------------KNN_5_inverse-------------------------\")\n",
    "print(classification_report(y_test, inverse_predictions_5, zero_division = 0))\n",
    "print(\"---------------------KNN_10_inverse-------------------------\")\n",
    "print(classification_report(y_test, inverse_predictions_10, zero_division = 0))\n",
    "print(\"---------------------KNN_1_majority-------------------------\")\n",
    "print(classification_report(y_test, majority_predictions_1, zero_division = 0))\n",
    "print(\"---------------------KNN_5_majority-------------------------\")\n",
    "print(classification_report(y_test, majority_predictions_5, zero_division = 0))\n",
    "print(\"---------------------KNN_10_majority-------------------------\")\n",
    "print(classification_report(y_test, majority_predictions_10, zero_division = 0))        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-percentage",
   "metadata": {},
   "source": [
    "*Answer Here*   \n",
    "\n",
    "\n",
    "We use KNN to estimate the accuracy of prediction based on inverse distance and majority voting respectively. It can be found out that accuracy increases as the k value increases. In K nearest neighbours, k, which is a crucial parameter while using it, indicates the number of nearest number we need to select. When k is small, the model may accidentally capture the noise and outliers and the model may be overfit in consequence. Thereby, large k value can ensure the better decision-making. In case of class distribution is imbalanced, using KNN model with inverse distance weighting can assign more weights to closer neighbours since it uses the distance from the neighbours.\n",
    "\n",
    "  \n",
    "Next, in terms of precision, recall and F1 score. Basiclly, KNN model with inverse distance weighting can generate more precise values of precision, recall and F1 compared to the model with majority.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-swing",
   "metadata": {},
   "source": [
    "**C)** How would standardisation impact the performance of your KNN models and Gaussian Naive Bayes model for the Adult dataset? **[1 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-grace",
   "metadata": {},
   "source": [
    "*Answer Here*   \n",
    "\n",
    "\n",
    "KNN model is sensitive to the scale of features since it mainly uses the distance to do decision-making. In case of scales of features are different and larger scale may result in the incorrect results since different scales enable the large scale determine the distance metric. However, using standardisation can reduce the negative impact since it can make all features are in similar scale.\n",
    "\n",
    "Gaussian Naive Bayes essentially uses the distribution of features to do computations and give the corresponding results. Therefore, standardisation may not be neccessarily required while using GNB model. There are still some impacts even though it is not neccessarily required. In case of different scales, standardisation has the potential to make the model work better. Also, it could potentially prevent the overfitting.\n",
    "\n",
    "Overall, standardisation is vital for KNN model but is not vital for Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2419db",
   "metadata": {},
   "source": [
    "## Question 5. Evaluation metrics [2 marks]\n",
    "\n",
    "**A)** Update the code in questions 2, 3, and 4 to compute the following metrics for the models listed below:\n",
    "\n",
    "- One-R \n",
    "- Gaussian NB \n",
    "- Categorical NB\n",
    "- 3-Nearest Neighbor model with Euclidean distance and majority voting \n",
    "\n",
    "Report their performance using the following two metrics\n",
    "- micro-averaged precision\n",
    "- macro-averaged precision \n",
    " \n",
    "Conversely, you can also choose to implement the same 10 rounds of train and test split (80% train, 20% test) as described in the questions 2,3, and 4 in the code block below and report the average scores for the micro-precision and macro-precision.\n",
    "\n",
    "**[0.5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ad0ce1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Dataset Evaluation results:\n",
      "Micro-p of One-R: 0.05\n",
      "Micro-p of GNB: 0.8\n",
      "Micro-p of CNB: 0.77\n",
      "Micro-p of KNN(3): 0.76\n",
      "Macro-p of One-R: 0.38\n",
      "Macro-p of GNB: 0.74\n",
      "Macro-p of CNB: 0.78\n",
      "Macro-p of KNN(3): 0.64\n",
      "Student Dataset Evaluation results:\n",
      "Micro-p of One-R: 0.27\n",
      "Micro-p of GNB: 0.17\n",
      "Micro-p of CNB: 0.37\n",
      "Micro-p of KNN(3): 0.21\n",
      "Macro-p of One-R: 0.75\n",
      "Macro-p of GNB: 0.35\n",
      "Macro-p of CNB: 0.6\n",
      "Macro-p of KNN(3): 0.24\n"
     ]
    }
   ],
   "source": [
    "def compare_eval(num_dataset, cat_dataset):\n",
    "    \n",
    "    OneR_microP_1 = []\n",
    "    GNB_microP_1 = []\n",
    "    CNB_microP_1 = []\n",
    "    KNN3_microP_1_majority = []\n",
    "\n",
    "    OneR_macroP_1 = []\n",
    "    GNB_macroP_1 = []\n",
    "    CNB_macroP_1 = []\n",
    "    KNN3_macroP_1_majority = []\n",
    "    ## your code here\n",
    "    runs = 10\n",
    "    test_portion = 0.2\n",
    "    scores = []\n",
    "    for i in range(runs):\n",
    "        random_state = i\n",
    "        X_train, X_test, y_train, y_test = train_test_split(num_dataset.iloc[:,:-1], num_dataset.iloc[:,-1]\n",
    "        , test_size = test_portion, random_state = random_state)\n",
    "        \n",
    "        # One-R\n",
    "        score, predict = one_R_model(X_train, X_test, y_train, y_test)\n",
    "        scores.append(score)\n",
    "        OneR_microP = precision_score(y_test, predict, average = 'micro', zero_division = 1)\n",
    "        OneR_macroP = precision_score(y_test, predict, average = 'macro', zero_division = 1)\n",
    "        OneR_microP_1.append(OneR_microP)\n",
    "        OneR_macroP_1.append(OneR_macroP)\n",
    "\n",
    "        # Gaussian Naive Bayes\n",
    "        gaussian_clf = GaussianNB()\n",
    "        gaussian_clf.fit(X_train, y_train)\n",
    "        gaussian_predictions = gaussian_clf.predict(X_test)\n",
    "        GNB_microP = precision_score(y_test, gaussian_predictions, average = 'micro', zero_division = 1)\n",
    "        GNB_macroP = precision_score(y_test, gaussian_predictions, average = 'macro', zero_division = 1)\n",
    "        GNB_microP_1.append(GNB_microP)\n",
    "        GNB_macroP_1.append(GNB_macroP)\n",
    "\n",
    "        # Categorical Naive Bayes\n",
    "        encoder = OrdinalEncoder()\n",
    "        labels = cat_dataset.iloc[:,-1]\n",
    "        labels_2d = labels.values.reshape(-1, 1)\n",
    "        labels_encode = encoder.fit_transform(labels_2d)\n",
    "        \n",
    "        new_dataset = cat_dataset.drop(columns = ['ID'])\n",
    "        header = []\n",
    "        for feature in new_dataset.columns[:-1]:\n",
    "            header.append(feature)\n",
    "        encoded = pd.get_dummies(new_dataset[header], drop_first=True)\n",
    "        train, test, label_train, label_test= train_test_split(encoded, labels_encode, test_size = test_portion, random_state = counter)\n",
    "        categorical_clf = CategoricalNB(min_categories=7)\n",
    "        new_train_label = label_train.ravel()\n",
    "        categorical_clf.fit(train, new_train_label)\n",
    "        categorical_predictions = categorical_clf.predict(test)\n",
    "        CNB_microP = precision_score(label_test.ravel(), categorical_predictions, average = 'micro', zero_division = 1)\n",
    "        CNB_macroP = precision_score(label_test.ravel(), categorical_predictions, average = 'macro', zero_division = 1)\n",
    "        CNB_microP_1.append(CNB_microP)\n",
    "        CNB_macroP_1.append(CNB_macroP)\n",
    "\n",
    "        # K-nearest neighbours with k = 3\n",
    "        k_NUM = 3\n",
    "        KNN_3_major = KNeighborsClassifier(n_neighbors = k_NUM)\n",
    "        KNN_3_major.fit(X_train.values, y_train)\n",
    "        KNN_3_predictions = KNN_3_major.predict(X_test.values)\n",
    "        KNN3_microP = precision_score(y_test, KNN_3_predictions, average = 'micro', zero_division = 1)\n",
    "        KNN3_macroP = precision_score(y_test, KNN_3_predictions, average = 'macro', zero_division = 1)\n",
    "        KNN3_microP_1_majority.append(KNN3_microP)\n",
    "        KNN3_macroP_1_majority.append(KNN3_macroP)\n",
    "    \n",
    "    print(\"Micro-p of One-R:\", np.mean(OneR_microP_1).round(2))\n",
    "    print(\"Micro-p of GNB:\", np.mean(GNB_microP_1).round(2))\n",
    "    print(\"Micro-p of CNB:\", np.mean(CNB_microP_1).round(2)) \n",
    "    print(\"Micro-p of KNN(3):\", np.mean(KNN3_microP_1_majority).round(2))\n",
    "\n",
    "    print(\"Macro-p of One-R:\", np.mean(OneR_macroP_1).round(2))\n",
    "    print(\"Macro-p of GNB:\", np.mean(GNB_macroP_1).round(2))\n",
    "    print(\"Macro-p of CNB:\", np.mean(CNB_macroP_1).round(2)) \n",
    "    print(\"Macro-p of KNN(3):\", np.mean(KNN3_macroP_1_majority).round(2))\n",
    "    \n",
    "\n",
    "##Adult Dataset and Student Dataset results: \n",
    "\n",
    "print(\"Adult Dataset Evaluation results:\")\n",
    "compare_eval(adult_num_dataset,adult_cat_dataset)\n",
    "\n",
    "print(\"Student Dataset Evaluation results:\")\n",
    "compare_eval(student_num_dataset,student_cat_dataset)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81f752",
   "metadata": {},
   "source": [
    "**B)** Compare the average accuracy vs. macro-average and micro-average precision for the two datasets. Explain which evaluation measurement would be most appropriate for each dataset **[1.5 mark]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-sleep",
   "metadata": {},
   "source": [
    "*Answer Here*   \n",
    "In adult data set, One-R and KNN(3) can acheive high accuracy and precision. Micro-average precision and macro - average precision may be treated as the most suitable measurements as dataset is relatively balanced and features are relatively dependent.   \n",
    "In student data set, gaussian naive bayes gives the highest micro-precision value and macro-precision value among these 4 models. In student data set, data is not well balanced. And by the definitions of micro-average and macro-average, former one maily calculates for each instance and latter one calculates for each class. Hence,  micro-average precision may be most appropriate.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-pollution",
   "metadata": {},
   "source": [
    "## Question 6. Ethics and implications in practice [4 marks]\n",
    "\n",
    "The Categorical Naive Bayes classifier you developed in this assignment for the student dataset could for example be used to classify college applicants into admitted vs not-admitted depending on their predicted grade in the Student dataset.\n",
    "\n",
    "**A)** Discuss ethical problems which might arise in this application and lead to unfair treatment of the applicants. Ground your discussion in the set of features provided in the student data set.**[1 marks]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-package",
   "metadata": {},
   "source": [
    "*Answer Here*  \n",
    "In the student data set. There are some features which are irrelevant to the final label class.  \n",
    "The first one is the sensitive information problem. The occupations of father and mother , which are sensitive information to student, even the whole family, are included as features. These features may cause the violation to the privacy In Categorical Naive Bayes model, these privacy features may lead to a wrong prediction.  \n",
    "The second one is the bias and potential discrimination. Machine learning algorithm will not identify if the train data set is biased or not. Thereby, in a case of a data set with bias towards a certain group of people, Categorical Naive Bayes may use the wrong data to unfairly to label other persons in this group.  \n",
    "The third problem should be addressed is access to education background. For example, final score of a student may not identified as low mark just because he/she graduated from a underprivileged high school or he/she did not participate many extracurricular activities.  \n",
    "These features, which may cause ethical problems, should be removed from the original data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-hudson",
   "metadata": {},
   "source": [
    "**B)** Remove all ethically problematic features from the data set (use your own judgment), and train your Naive Bayes classifiers on the resulting data set. How does the performance change in comparison to the full classifier ( consider accuracy and micro-average precision)?**[2 marks]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "conventional-heaven",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of full BNB: 0.31\n",
      "full micro precision: 0.31\n",
      "full macro precision: 0.28\n",
      "\n",
      "Accuracy of new BNB: 0.33\n",
      "updated micro precision: 0.33\n",
      "updated macro precision: 0.31\n"
     ]
    }
   ],
   "source": [
    "## your code here for part B\n",
    "def new_preprocess(fileName,num_feat, removed):\n",
    "    data = pd.read_csv(fileName)\n",
    "    categorical_headers = []\n",
    "\n",
    "    ## remove columns that may cause ethical problem\n",
    "    for a in removed:\n",
    "        data = data.drop(columns = [a])\n",
    "    \n",
    "    ## replace missing values with the most frequent for categorical feature\n",
    "    for feature in data.columns[1:-1]:\n",
    "        if feature in num_feat:\n",
    "            data[feature] = pd.to_numeric(data[feature], errors='coerce')\n",
    "            mean = data[feature].mean()\n",
    "            data[feature].fillna(mean, inplace=True)\n",
    "    \n",
    "    ## replace missing values with the average for numerical features\n",
    "        else:\n",
    "            categorical_headers.append(feature)\n",
    "            mode_values = data[feature].mode()\n",
    "            mode_value = mode_values[0] # We usually pick the first value in case of there are multiple values with same frequency\n",
    "            data.loc[data[feature] == \"?\", feature] = mode_value\n",
    "\n",
    "    ## convert categorical features to numeric using one-hot encoding\n",
    "    dataset = data.copy()\n",
    "    encoder = OrdinalEncoder()\n",
    "    label_data = (dataset.iloc[:, -1]).values\n",
    "    label_data = label_data.reshape(-1,1)\n",
    "    encoded_label = pd.DataFrame(encoder.fit_transform(label_data))\n",
    "    encoded = pd.get_dummies(dataset[categorical_headers], drop_first=True)\n",
    "    num_dataset = pd.concat([dataset[num_feat], encoded, encoded_label], axis = 1)\n",
    "    return num_dataset\n",
    "\n",
    "removed_columns = ['Mjob', 'Fjob', 'sex']\n",
    "new_student_num_set = new_preprocess(\"datasets/student.csv\",[], removed_columns)\n",
    "\n",
    "test_portion = 0.2\n",
    "runs = 10\n",
    "BNB_Acc_1 = [] \n",
    "micro_P = []\n",
    "macro_P = []  \n",
    "full_BNB_Acc_1 = [] \n",
    "full_micro_P = []\n",
    "full_macro_P = []  \n",
    "for i in range(runs):\n",
    "    counter = i\n",
    "    X_train, X_test, y_train, y_test = train_test_split(new_student_num_set.iloc[:,:-1], new_student_num_set.iloc[:, -1], train_size = 1 - test_portion, test_size = test_portion, random_state = counter)\n",
    "    x_train, x_test, Y_train, Y_test = train_test_split(student_num_dataset.iloc[:,:-1], student_num_dataset.iloc[:, -1], train_size = 1 - test_portion, test_size = test_portion, random_state = counter)\n",
    "    \n",
    "    ## full Bernoulli Naive Bayes\n",
    "    bernoulli_clf_full = BernoulliNB()\n",
    "    bernoulli_clf_full.fit(x_train, Y_train)\n",
    "    bernoulli_predictions_full = bernoulli_clf_full.predict(x_test)\n",
    "    micro_full = precision_score(Y_test, bernoulli_predictions_full, average = 'micro', zero_division = 1)\n",
    "    macro_full = precision_score(Y_test, bernoulli_predictions_full, average = 'macro', zero_division = 1)\n",
    "    full_micro_P.append(micro_full)\n",
    "    full_macro_P.append(macro_full)\n",
    "    bernoulli_acc_full = accuracy_score(Y_test, bernoulli_predictions_full)\n",
    "    full_BNB_Acc_1.append(bernoulli_acc_full)\n",
    "    \n",
    "    ## new Bernoulli Naive Bayes\n",
    "    bernoulli_clf = BernoulliNB()\n",
    "    bernoulli_clf.fit(X_train, y_train)\n",
    "    bernoulli_predictions = bernoulli_clf.predict(X_test)\n",
    "    micro = precision_score(y_test, bernoulli_predictions, average = 'micro', zero_division = 1)\n",
    "    macro = precision_score(y_test, bernoulli_predictions, average = 'macro', zero_division = 1)\n",
    "    micro_P.append(micro)\n",
    "    macro_P.append(macro)\n",
    "    bernoulli_acc = accuracy_score(y_test, bernoulli_predictions)\n",
    "    BNB_Acc_1.append(bernoulli_acc)\n",
    "\n",
    "print(\"Accuracy of full BNB:\", np.mean(full_BNB_Acc_1).round(2))  \n",
    "print(\"full micro precision:\", np.mean(full_micro_P).round(2))\n",
    "print(\"full macro precision:\", np.mean(full_macro_P).round(2))\n",
    "print()\n",
    "print(\"Accuracy of new BNB:\", np.mean(BNB_Acc_1).round(2))  \n",
    "print(\"updated micro precision:\", np.mean(micro_P).round(2))\n",
    "print(\"updated macro precision:\", np.mean(macro_P).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7067969a",
   "metadata": {},
   "source": [
    "*Answer Here*\n",
    "\n",
    "After deleting some columns, the accuracy increases but not substantially, it is slow growth. Value of micro-average precision grows slowly as well.\n",
    "\n",
    "It means that number of features can affect the performance of BNN but not highly affect the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-greeting",
   "metadata": {},
   "source": [
    "**C)** The approach to fairness we have adopted is called “fairness through unawareness”, where we simply deleted any questionable features from our data. Is removing all problematic features as done in part (b) guarantee a fair classifier? Explain Why or Why not?**[1 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-chester",
   "metadata": {},
   "source": [
    "*Answer Here*\n",
    "\n",
    "Even though some features, which may result in ethical problems, are deleted. The overall performance does not improve substantially. It means that this approach cannot guarantee the fairness and classifiers still may be able to produce unfair predictions. The algorithm we choose to do classification may learn the hidden bias and then produce the wrong results. Hidden correlations may not be removed and other indirect bias are still existing in the dataset. Sometimes, if the dataset is complex and then the result could be affected since some significant information, which could lead to fairer result, is removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55940e72-3414-4dab-be69-96a579354617",
   "metadata": {},
   "source": [
    "# Authorship Declaration:\n",
    "\n",
    "   (1) I certify that the program contained in this submission is completely\n",
    "   my own individual work, except where explicitly noted by comments that\n",
    "   provide details otherwise.  I understand that work that has been developed\n",
    "   by another student, or by me in collaboration with other students,\n",
    "   or by non-students as a result of request, solicitation, or payment,\n",
    "   may not be submitted for assessment in this subject.  I understand that\n",
    "   submitting for assessment work developed by or in collaboration with\n",
    "   other students or non-students constitutes Academic Misconduct, and\n",
    "   may be penalized by mark deductions, or by other penalties determined\n",
    "   via the University of Melbourne Academic Honesty Policy, as described\n",
    "   at https://academicintegrity.unimelb.edu.au.\n",
    "\n",
    "   (2) I also certify that I have not provided a copy of this work in either\n",
    "   softcopy or hardcopy or any other form to any other student, and nor will\n",
    "   I do so until after the marks are released. I understand that providing\n",
    "   my work to other students, regardless of my intention or any undertakings\n",
    "   made to me by that other student, is also Academic Misconduct.\n",
    "\n",
    "   (3) I further understand that providing a copy of the assignment\n",
    "   specification to any form of code authoring or assignment tutoring\n",
    "   service, or drawing the attention of others to such services and code\n",
    "   that may have been made available via such a service, may be regarded\n",
    "   as Student General Misconduct (interfering with the teaching activities\n",
    "   of the University and/or inciting others to commit Academic Misconduct).\n",
    "   I understand that an allegation of Student General Misconduct may arise\n",
    "   regardless of whether or not I personally make use of such solutions\n",
    "   or sought benefit from such actions.\n",
    "\n",
    "   <b>Signed by</b>: [Wang Risheng]\n",
    "   \n",
    "   <b>Dated</b>: [30/08/2023]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
