{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766bbbe5-adcc-403b-b220-e400c21cb7fa",
   "metadata": {},
   "source": [
    "# Assignment 1: Binary Decision Tree (10 marks)\n",
    "\n",
    "Student Name: Wang Risheng\n",
    "\n",
    "Student ID: 1053051\n",
    "\n",
    "## General info\n",
    "\n",
    "<b>Due date</b>: Friday, 11 August 2023 at 5 pm \n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this Jupyter notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day up to 5 days (both weekdays and weekends count)\n",
    "<ul>\n",
    "    <li>one day late, -1.0;</li>\n",
    "    <li>two days late, -2.0;</li>\n",
    "    <li>three days late, -3.0;</li>\n",
    "    <li>four days late, -4.0;</li>\n",
    "    <li>five days late, -5.0;</li>\n",
    "</ul>\n",
    "\n",
    "<b>Marks</b>: This assignment will be marked out of 10, and make up 10% of your overall mark for this subject.\n",
    "\n",
    "<b>Materials</b>: See [Using Jupyter Notebook and Python page] on Canvas (under Modules> Coding Resources) for information on the basic setup required for this class, including a Jupyter notebook viewer and the python packages `numpy` and `pprint`. You can use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "You will be marked not only on the correctness of your methods and answers but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it. We reserve the right to deduct up to 1 marks for unreadable or exessively inefficient code.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on Canvas>Assignments>Assignmnet1; we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: This assignment is an individual task, and so reuse of code or other instances of clear influence will be considered cheating. Please check the <a href=\"https://canvas.lms.unimelb.edu.au/courses/153706/modules#module_662096\">CIS Academic Honesty training</a> for more information. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "Please carefully read and fill out the <b>Authorship Declaration</b> form at the bottom of the page. Failure to fill out this form results in the following deductions: \n",
    "<UL TYPE=”square”>\n",
    "<LI>Missing Authorship Declaration at the bottom of the page, -1.0\n",
    "<LI>Incomplete or unsigned Authorship Declaration at the bottom of the page, -0.5\n",
    "</UL>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01e21e-727c-4b91-8bb9-0e98bf7f085d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this assignment, you will build the binary Decision Tree (DT) classification algorithm from scratch and apply it to a real-world machine learning dataset. You are not allowed to use third party library functions and directly call pre-built DT algorithms. After building the DT, you will predict the predominant color of national flags using a diverse set of features, including country features such as language, population, in addition to other structural properties. The list of classes (colors) is:\n",
    "\n",
    "```\n",
    "black\n",
    "blue\n",
    "brown\n",
    "gold\n",
    "green\n",
    "orange\n",
    "red\n",
    "white\n",
    "```\n",
    "\n",
    "We use a modified version of the Flags data set from the UCI Machine learning repository <a href=\"https://archive.ics.uci.edu/ml/datasets/Flags\"> Flags Dataset</a> (click the link for more information on all features and values).\n",
    "\n",
    "\n",
    "In the **Data Preparation** section, you will read the dataset into a data frame(Q1.1). You will also need to develop code to convert two of the numeric features to nominal (Q1.2). Then, the data will be divided into two parts: Train and Test. (Q1.3)\n",
    "\n",
    "In the **Model Training** section, you will write a number of helper functions that will be incorporated into the main Binary Decision Tree function (Q2-6).\n",
    "\n",
    "You can observe the use of each of these functions in the Main Binary Decision Tree Algorithm, which you can use to print your trained tree. We have also provided the code for navigating a trained tree and using it to predict the labels for the test dataset.\n",
    "\n",
    "In the **Classification and Evaluation** section, you will test and evaluate your model. You will implement functions from scratch to calculate the accuracy of the model predictions (Q7). *For the evaluation part you are NOT allowed to use third party provided library functions such as classification_report.* Finally, you will train a tree and use it to predict the labels for the test set and see the accuracy of your model (Q8).\n",
    "\n",
    "Q9 includes some analytical questions in which you will critically analyze the behaviour of your tree in different situations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "6c8b6c34-1376-4b46-96fb-91cd09f03767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c063576-3b53-4516-a4d3-c3caddb38051",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "The dataset that you have consist of a dataset with 194 flags. Each flag is defined by 26 features and 1 `label`. These features are `index`, `landmass`,\t`zone`,\t`area`,\t..., `animate`,\tand `text`.\n",
    "The first feature is only an index and is not useful for classification, but the rest of the features can be used for predicting the color of the flag, i.e., `label`. \n",
    "\n",
    "\n",
    "#### Q1.1 Reading the input file and data preprocessing (0.25 marks)\n",
    "\n",
    "This code should read the input file into a `pandas` dataframe. You should remove the first feature because it is only an id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "e7b54cc5-44ae-43f1-ae76-33386b566a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ YOUR CODE HERE ##############\n",
    "# First, read the input file\n",
    "df = pd.read_csv(\"flags.data.csv\")\n",
    "# Second, drop the index column\n",
    "df = df.drop(columns='index')\n",
    "\n",
    "############## TEST IT YOURSELF ###############\n",
    "\n",
    "assert df['landmass'][0] == 5\n",
    "assert df['zone'][67] == 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e96191-5b43-41d6-8e9b-10204d044967",
   "metadata": {},
   "source": [
    "#### Q1.2 Convert Numeric features to Categorical (0.5 marks)\n",
    "In order to simplify our calculations we are going to transform the `area` and `population` features with numeric values to categories. You will be using **equal frequency binning with 5 bins**.\n",
    "\n",
    "**NOTE:** Assign sequential integer values starting from 0 to 4 to each of the bins. For example for `population` feature, any value between [0,1] is mapped to 0, any value between (1,4] is mapped to 1, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "26261816-948d-4901-9429-3f0d1ba9c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ YOUR CODE HERE ##############\n",
    "def equal_frequency_area(data):\n",
    "    bin_size = 5\n",
    "    df.sort_values('area')\n",
    "    value = pd.qcut(df['area'], bin_size, labels = False, duplicates = 'drop')\n",
    "    return value\n",
    "\n",
    "def equal_frequency_population():\n",
    "    bin_size = 5\n",
    "    \n",
    "    value = pd.qcut(df['population'], bin_size + 1, labels = False, duplicates = 'drop') \n",
    "    return value\n",
    "\n",
    "## It shows that we cannot use qcut function directly to do equal frquency binning of 5 bins for values of population\n",
    "## We may manually do the binning by using a stable sort algorithm\n",
    "## Selection sort may be a good option since it is easy to use\n",
    "## We not only swap the values but also the indices in the original array\n",
    "## and we put indices in an array called indices\n",
    "def insertion_sort_to_find_indices(array):\n",
    "    indices = []\n",
    "\n",
    "    for i in range(len(array)):\n",
    "        indices.append(i)\n",
    "\n",
    "    for i in range(len(array)):\n",
    "        min_index = i\n",
    "        for j in range(i+1, len(array)):\n",
    "            if array[min_index] > array[j]:\n",
    "                min_index = j\n",
    "            array[i], array[min_index] = array[min_index], array[i]     \n",
    "        ##print(min_index)\n",
    "        indices[i], indices[min_index] = indices[min_index], indices[i]\n",
    "    return indices\n",
    "\n",
    "## Now we aim to separate the indices after swapping into 5 bins and each bin size has almost same size\n",
    "array = df['population'].values\n",
    "indices = insertion_sort_to_find_indices(array) \n",
    "bin_number = 5\n",
    "each_bin_size = (len(array) // bin_number) + 1 ## Maximum size for each bin\n",
    "\n",
    "indices_for_bin = [[] for _ in range(bin_number)] ## create an empty 2d array with 5 inner arrays\n",
    "index = 0\n",
    "run = True ## boolean value to indicate whether run\n",
    "i = 0\n",
    "while i in range(bin_number) and run:\n",
    "    j = 0\n",
    "    while j < each_bin_size:\n",
    "        if index < len(indices):\n",
    "            indices_for_bin[i].append(indices[index])\n",
    "            index += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            run = False\n",
    "            break;  \n",
    "    i += 1\n",
    "\n",
    "# Assign bin labels to the original data\n",
    "\n",
    "## Assign bin labels to the original area data\n",
    "df['area'] = equal_frequency_area(df['area'])\n",
    "## Assign bin labels to the original population data\n",
    "for i in range(len(indices_for_bin)):\n",
    "    j = 0\n",
    "    while j < len(indices_for_bin[i]):\n",
    "        df.loc[indices_for_bin[i][j], 'population'] = i\n",
    "        j += 1\n",
    "    i += 1\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77221c-1587-4b93-bd4d-19d7dc97ede8",
   "metadata": {},
   "source": [
    "#### Q1.3 Split the data into a Train and Test Set (0.25 marks)\n",
    "The first 150 instances should be used for training and the last 44 instances for testing. **SHUFFLING IS NOT ALLOWED!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "fe01b801-a6d4-447b-811e-4517e7e7e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 150 instances should be used for training and the last 44 instances for testing\n",
    "# SHUFFLING IS NOT ALLOWED\n",
    "\n",
    "train_df = df[:150]\n",
    "test_df = df[150:]\n",
    "\n",
    "############## TEST IT YOURSELF ###############\n",
    "assert(len(train_df)==150)\n",
    "assert(len(test_df)==44)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-danish",
   "metadata": {},
   "source": [
    "## Train a Binary Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd7f440-8152-43cc-8e63-805f4ac49401",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "There are a number of helper functions that we provide here. In addition to the following helper functions, you must implement a few other helper functions to use in your binary decision tree algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "streaming-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# This function receives instances in a 2D array format \n",
    "# and returns the most common label for the passed instances. \n",
    "\n",
    "def assign_label(data):\n",
    "    # the input `data` is a 2D array\n",
    "    # the output is a STRING \n",
    "    \n",
    "    column = data[:, -1] \n",
    "    \n",
    "    values, counts = np.unique(column, return_counts=True)\n",
    "    index = counts.argmax()\n",
    "    name = values[index]\n",
    "    \n",
    "    return name\n",
    "\n",
    "#######################################################################################\n",
    "# This function separates instances (data) based on a given feature (split feature)\n",
    "# and a threshold value (value). \n",
    "# it returns a left and right subtree corresponding to that threshold value.\n",
    "\n",
    "def find_subtrees(data, features_list, split_feature, value):  \n",
    "    # the input `data` is a 2D array \n",
    "    # the input `features_list` is the list of feature names\n",
    "    # the input `split_feature` specifies a single feature name\n",
    "    # the input `value` specifies a value for the `split_feature`\n",
    "    # the outputs `data_left` and `data_right` are 2D arrays (subtrees) \n",
    "    \n",
    "    index = features_list.get_loc(split_feature)\n",
    "    data_left = data[data[:,index]<value]\n",
    "    data_right = data[data[:,index]>=value]\n",
    "    \n",
    "    return data_left, data_right\n",
    "\n",
    "#######################################################################################\n",
    "# This function computes the entropy of a set of instances (data) \n",
    "\n",
    "def calc_entopy(data):\n",
    "    # the input `data` is a 2D array \n",
    "    # the output is a single real-valued number\n",
    "    \n",
    "    column = data[:, -1]\n",
    "    _, counts = np.unique(column, return_counts=True)\n",
    "\n",
    "    x = counts / counts.sum()  \n",
    "    y = sum(x * -np.log2(x))\n",
    "    \n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759aba23-69eb-4e62-8f91-4669f98ee9a7",
   "metadata": {},
   "source": [
    "**Q2. Write a function that will determine whether a node is pure or not. (0.5 marks)**\n",
    "\n",
    "It takes as input: \n",
    "- data: 2D array of values containing instances\n",
    "\n",
    "It returns as output: \n",
    "- answer: a boolean value indicating whether the data (node) is pure or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "b2e7fe64-9769-47d0-bd19-b0cf976cf77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function receives a 2D array of values containing instances, it should have a format such as follows:\n",
    "#  [[3, 1, 1, ..., 1, 0, 'red'],\n",
    "#   [4, 1, 4, ..., 0, 0, 'green'],\n",
    "#   [1, 1, 0, ..., 0, 0, 'red'],\n",
    "#   ...,\n",
    "#   [5, 1, 3, ..., 0, 0, 'red']]\n",
    "\n",
    "\n",
    "def calc_purity(data): \n",
    "    # the input `data` is a 2D array as illustrated above\n",
    "    # the output `answer` is a BOOLEAN (True or False)\n",
    "    \n",
    "    ############ YOUR CODE HERE ##############\n",
    "    value = calc_entopy(data)\n",
    "    if value != 0:\n",
    "        answer = False\n",
    "    else:\n",
    "        answer = True\n",
    "    \n",
    "    ##########################################\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "8e6b3aa1-488d-4ba6-80a4-368695557064",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TEST IT YOURSELF ###############\n",
    "assert calc_purity(df.values) == False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09950a8-08d0-4129-88b2-785d1fc635ff",
   "metadata": {},
   "source": [
    "**Q3. Write a function that will determine the mean information for a given set of instances. (0.75 marks)**\n",
    "\n",
    "It takes as input: \n",
    "- data: an array of 2D arrays of values containing instances\n",
    "\n",
    "It returns as output: \n",
    "- mi: mean information which is a single real-valued number, refer to lecture 4 slides for Mean info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "105b666c-c5b0-4e3a-8ea1-3b98320675a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function receives an array of 2D arrays of values containing instances, it should have a format such as follows:\n",
    "# [[[3, 1, 1, ..., 1, 0, 'red'],\n",
    "#   [4, 1, 4, ..., 0, 0, 'green'],\n",
    "#   [1, 1, 0, ..., 0, 0, 'red'],\n",
    "#   ...,\n",
    "#   [5, 1, 3, ..., 0, 0, 'red'],\n",
    "\n",
    "#   [[6, 3, 0, ..., 1, 0, 'blue'],\n",
    "#    [2, 3, 4, ..., 0, 0, 'gold'],\n",
    "#    [2, 3, 4, ..., 0, 0, 'blue'],\n",
    "#    ...,\n",
    "#    [3, 3, 3, ..., 0, 0, 'blue']]]\n",
    "\n",
    "\n",
    "def calculate_MI(data):\n",
    "    # the input `data` is an array of 2D arrays as illustrated above\n",
    "    # the output `mi` is a real-valued number\n",
    "    ############ YOUR CODE HERE ##############\n",
    "    total_array = 0\n",
    "    for a in data:\n",
    "        total_array = total_array + len(a)\n",
    "    \n",
    "    mi = 0\n",
    "    for a in data:\n",
    "        entropy = calc_entopy(a)\n",
    "        prob = len(a)/total_array\n",
    "        mi += entropy * prob\n",
    "    \n",
    " \n",
    "    ##########################################\n",
    "    \n",
    "    return mi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "central-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TEST IT YOURSELF ###############\n",
    "assert(calculate_MI([df[df['label']=='red'].values,df[df['label']=='red'].values]) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be04ccf-9afa-4c30-8457-3092d553ff35",
   "metadata": {},
   "source": [
    "**Q4. Write a function that will determine the information gain after we perform a split for a feature (0.5 marks)**\n",
    "\n",
    "It takes as input: \n",
    "-  root: the data before the split\n",
    "-  children: the subtrees after the split\n",
    "\n",
    "\n",
    "It returns as output: \n",
    "- ig: Information Gain which is a single real-valued number, please refer to lecture 4 slides for information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "d641b5a1-1c29-4b22-b385-92b807818ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IG(root, children):\n",
    "    # the input `root` is a 2D array as illustrated in Q2\n",
    "    # the input `children` is an array of 2D arrays as illustared in Q3\n",
    "    # the output `ig` is a real-valued number\n",
    "    ############ YOUR CODE HERE ##############\n",
    "    root_entropy = calc_entopy(root)\n",
    "    children_MI = calculate_MI(children)\n",
    "    ig = root_entropy - children_MI \n",
    "    \n",
    "    ##########################################\n",
    "    \n",
    "    return ig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-marijuana",
   "metadata": {},
   "source": [
    "**Q5. Write a function that will determine the best feature and value combination that results in the highest information gain (0.75 marks)**\n",
    "\n",
    "It takes as input: \n",
    "-  data: the data before the split\n",
    "-  features_list: the list of feature names\n",
    "\n",
    "\n",
    "It returns as output: \n",
    "- y1: name of the feature with the best information gain ( String ) \n",
    "- y2: value of the feature with the best information gain ( a feature value )\n",
    "\n",
    "**Note: use the find_subtrees(data, feature_list, split_feature, value) function for all possible combinations of <feature,value> to determine the best <feature,value> combination resulting in the highest information gain** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "young-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_IG(data,features_list):\n",
    "    # the input `data` is a 2D array as illustrated in Q2\n",
    "    # the input `feature_list` is a list of feature names in the format of pandas Index\n",
    "    # the output `y1` is a String\n",
    "    # the output y2 is a numeric feature value\n",
    "    ############ YOUR CODE HERE ##############\n",
    "    max = 0\n",
    "    \n",
    "    for a in features_list[0:-1]:\n",
    "        values = df[a]\n",
    "        for i in values:\n",
    "            left, right = find_subtrees(data, features_list, a, i)\n",
    "            if calculate_IG(data,[left, right]) > max:\n",
    "                max = calculate_IG(data,[left, right])\n",
    "                y1 = a\n",
    "                y2 = i\n",
    "    \n",
    "    \n",
    "    ##########################################\n",
    "    \n",
    "    return y1, y2\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "interior-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TEST IT YOURSELF ###############\n",
    "assert(find_best_IG(train_df.values,train_df.columns)==('blue',1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68178b9f-0119-42b4-b534-a8fe401601a1",
   "metadata": {},
   "source": [
    "### Main Binary Decision Tree Algorithm\n",
    "**Q6. This is the recursive part of developing the Binary Decision Tree. If you have developed all the previous parts correctly this function will make a Binary Decision Tree for the passed dataset. We can set the maximum depth of the tree (`max_depth`) and number of samples that we would stop splitting a node (`min_samples`). You must edit the code below in the marked area `criteria` for the stopping condition in the ID3 algorithm. (0.5 marks)**\n",
    "\n",
    "The criteria are:\n",
    " - the current node is pure or;\n",
    " - number of instances in the current node is less than minimum samples or;\n",
    " - we have reached the maximum depth of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "20cf002d-8acc-46b6-a481-b1619a2927fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_algorithm(data, array, max_depth, counter=0, min_samples=2):\n",
    "    \n",
    "    \n",
    "    criteria = calc_purity(data) == True or len(array) < min_samples ############  YOUR CODE HERE  ##############  \n",
    "    if criteria :\n",
    "        y = assign_label(data)\n",
    "        return y\n",
    "    \n",
    "    # recursive part\n",
    "    else:\n",
    "        counter += 1\n",
    "        \n",
    "        name, value = find_best_IG(data,array)\n",
    "        data1, data2 = find_subtrees(data, array, name, value)\n",
    "        \n",
    "        if (len(data1) == 0) or (len(data2) == 0):\n",
    "            y = assign_label(data)\n",
    "            return y\n",
    "    \n",
    "        #instansiate the tree\n",
    "        node = \"{} < {}\".format(name, value)\n",
    "        sub_tree = {node: []}\n",
    "              \n",
    "        #develop the sub-trees (recursion)\n",
    "        left_child = decision_tree_algorithm(data=data1, array=array, max_depth=max_depth, counter=counter)\n",
    "        right_child = decision_tree_algorithm(data=data2, array=array, max_depth=max_depth, counter=counter)\n",
    "        \n",
    "        \n",
    "        if left_child == right_child:\n",
    "            sub_tree = right_child\n",
    "        else:\n",
    "            sub_tree[node].append(left_child)\n",
    "            sub_tree[node].append(right_child)\n",
    "        \n",
    "        return sub_tree   \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "linear-protein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blue < 1': [{'red < 1': [{'orange < 1': [{'population < 2': ['white', 'green']}, {'landmass < 5': [{'landmass < 4': ['white', 'orange']}, 'brown']}]}, {'gold < 1': [{'area < 3': [{'circles < 4': [{'white < 1': [{'zone < 2': ['green', {'landmass < 3': ['black', 'red']}]}, {'triangle < 1': [{'colours < 4': ['red', {'area < 2': [{'stripes < 3': ['red', 'green']}, 'red']}]}, 'black']}]}, 'orange']}, {'area < 4': [{'language < 8': ['white', {'green < 1': ['white', 'red']}]}, {'circles < 1': [{'colours < 3': ['red', {'landmass < 5': ['green', 'red']}]}, 'white']}]}]}, {'green < 1': [{'area < 3': [{'zone < 3': [{'population < 2': [{'sunstars < 1': ['gold', 'red']}, 'red']}, 'white']}, {'area < 4': ['black', {'stripes < 3': ['red', 'black']}]}]}, {'zone < 2': [{'language < 6': ['gold', 'green']}, {'area < 3': [{'area < 2': [{'zone < 3': ['red', 'gold']}, 'green']}, {'bars < 2': [{'stripes < 5': ['red', 'gold']}, 'gold']}]}]}]}]}]}, {'red < 1': [{'green < 1': [{'stripes < 3': [{'stripes < 1': [{'area < 3': [{'orange < 1': ['blue', 'brown']}, 'white']}, 'white']}, 'blue']}, {'landmass < 5': ['green', 'orange']}]}, {'white < 1': [{'icon < 1': [{'population < 4': ['gold', 'red']}, 'red']}, {'population < 2': [{'colours < 4': [{'crosses < 1': [{'stripes < 3': ['white', 'red']}, {'area < 1': ['white', 'blue']}]}, {'orange < 1': [{'stripes < 1': [{'landmass < 2': [{'crosses < 1': ['green', 'red']}, {'landmass < 5': [{'area < 3': ['blue', 'gold']}, 'gold']}]}, {'language < 10': ['red', 'green']}]}, 'blue']}]}, {'saltires < 1': [{'quarters < 1': [{'colours < 5': [{'zone < 2': [{'language < 10': ['red', {'population < 3': ['red', 'blue']}]}, {'area < 2': [{'landmass < 3': ['red', 'blue']}, 'blue']}]}, 'red']}, 'red']}, 'blue']}]}]}]}]}\n"
     ]
    }
   ],
   "source": [
    "############## TEST IT YOURSELF ###############\n",
    "data = train_df\n",
    "tree = decision_tree_algorithm(data=data.values, array=data.columns, max_depth=2, counter=0)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9546eb99-134d-412e-9017-bcb9653bff3d",
   "metadata": {},
   "source": [
    "# Classification and Evaluation\n",
    "\n",
    "Now that we have built our binary decision tree, we are ready to use it to predict a label for unseen instances. We provided the code to do this, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "d08da855-4902-4f7b-8aab-99c930a96f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_instance(example, tree):\n",
    "    \n",
    "    # tree is just a root node\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    \n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split(\" \")\n",
    "\n",
    "    # ask question\n",
    "    if (example[feature_name] < float(value)):\n",
    "        answer = tree[question][0]\n",
    "    else:\n",
    "        answer = tree[question][1]\n",
    "\n",
    "    # base case (the answer in not a dictionary)\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    \n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return predict_one_instance(example, residual_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "bb05533a-9ec4-46d1-99a4-534e3afdcf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(df, tree):\n",
    "    \n",
    "    if len(df) != 0:\n",
    "        predictions = df.apply(predict_one_instance, args=(tree,), axis=1)\n",
    "    else:\n",
    "        # \"df.apply()\"\" with empty dataframe returns an empty dataframe,\n",
    "        # but \"predictions\" should be a series instead\n",
    "        predictions = pd.Series()  \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c2c0f-527c-4f53-adfa-d68394413e61",
   "metadata": {},
   "source": [
    "**Q7. Write a function to calculate accuracy (0.25 marks)**\n",
    "\n",
    "This function receives a dataframe and predictions of a trained tree (on that dataset) and should calculate the accuracy of the predictions for the passed dataframe.\n",
    "\n",
    "**NOTE:** You are NOT allowed to use any predefined Python accuracy methods here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "d63b0013-addb-41d1-a2d6-779e7980c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df, predictions):\n",
    "    \n",
    "    ########### YOUR CODE HERE ##############\n",
    "    failure = 0\n",
    "    for true_value, prediction in zip(df['label'], predictions):\n",
    "        if true_value != prediction:\n",
    "            failure += 1\n",
    "    accuracy = 1 - failure / len(predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b9734-4118-44c4-a0b8-4b3d4f71e842",
   "metadata": {},
   "source": [
    "**Q8. Use the Tree (0.25 marks)**\n",
    "\n",
    "Using all the developed functions above build a tree using the **Training set** (train_df) (max_depth = 3) and then use it to predict labels for the **Test set** (test_df). Then print the accuracy of your tree for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "344aeeaf-d06c-447f-a034-357d060fe8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43181818181818177\n"
     ]
    }
   ],
   "source": [
    "#############  YOUR CODE HERE ##############\n",
    "max_depth = 3\n",
    "tree =  decision_tree_algorithm(train_df.values, train_df.columns, max_depth, counter=0)\n",
    "prediction = make_predictions(test_df, tree)\n",
    "accuracy = calculate_accuracy(test_df, prediction)\n",
    "print(accuracy)\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7537f2-d946-42fd-90a3-74a035e46819",
   "metadata": {},
   "source": [
    "## Q9. Analytical questions (5.5 marks)\n",
    "\n",
    "Answer each of the following subquestions with a text answer of 3-5 sentences, using the results you obtained in the previous sections. You might need to re-run and reuse some of the functions you already implemented in the ### CODE ### cells above.\n",
    "\n",
    "\n",
    "### Q9.1. Analysing equal-frequency versus equal-width binning (1.5 marks)\n",
    "\n",
    "**(A)** Write a code to transform the `area` and `population` features with numeric values to categories using **equal width** binning with 5 bins. **[0.5 mark]**\n",
    "\n",
    "**(B)** Name which approach is more appropriate for `area` and `population` features, and explain the reasoning behind your decision.\n",
    "**[1 mark]**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "pressing-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE 9.1(A) ###\n",
    "## First we find the maximum value and minimum value for area and population respectively\n",
    "def max_min(df, label):\n",
    "\n",
    "    max = df[label].values.max()\n",
    "    min = df[label].values.min()\n",
    "\n",
    "    return max,min\n",
    "\n",
    "area_max, area_min = max_min(df, 'area')\n",
    "population_max, population_min = max_min(df, 'population')\n",
    "\n",
    "## Now we use linspace to find the boundaries\n",
    "bin_size = 5\n",
    "area_boundaries = np.linspace(area_min, area_max, bin_size + 1)\n",
    "population_boundaries = np.linspace(population_min, population_max, bin_size + 1)\n",
    "\n",
    "area_weight = pd.cut(df['area'], area_boundaries, labels=range(1, bin_size + 1), include_lowest=True)\n",
    "population_weight = pd.cut(df['population'], population_boundaries, labels=range(1, bin_size + 1), include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-sunrise",
   "metadata": {},
   "source": [
    "**Type your text answers here**\n",
    "\n",
    "**(B)**\n",
    "\n",
    "Equal width binnning with 5 bins is a better approach for population and area.  \n",
    "The maximum value of area shown in the csv file is 22402 but majority of values are below 1000. If we choose equal frequency binning with 5 bins, the range of values in 1 bin is very large while that in another bin is small. In this case, it is hard to find the actual boundary.\n",
    "Similar for population. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-funeral",
   "metadata": {},
   "source": [
    "### Q9.2. Analysing model accuracy (2.25 marks)\n",
    "\n",
    "**(A)** Plot a <a href=\"https://en.wikipedia.org/wiki/Histogram\">histogram</a> of the actual class frequencies in the test set, and a histogram of the predicted test labels for the decision tree with depth=3. You should produce **a single plot** which shows the histogram both true and predicted labels. You should label the x-axis and y-axis appropriately and use legends to make your plot readable. [*N.B. you may use libraries like <a href=\"https://matplotlib.org/stable/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py\">matplotlib</a> or <a href=\"https://seaborn.pydata.org/introduction.html\">seaborne</a>*] **[1 mark]**\n",
    "\n",
    "**(B)** Describe and explain the discrepancy between the true and predicted distributions. **[0.75 mark]**\n",
    "\n",
    "**(C)** Do you think the accuracy is an appropriate evaluation metric for the *Flags* data set? Explain your answer. **[0.5 marks]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "conscious-programmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "################################################\n",
    "# Your answer to Question 9.2(A) STARTS HERE\n",
    "################################################\n",
    "# Example true test labels and predicted test labels (replace with your actual data)\n",
    "label1 = 'true' \n",
    "label2 = 'predicted'  \n",
    "data_label1 = test_df['label']\n",
    "data_label2 = prediction\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data_label1, bins=20, alpha=0.9, color='red', label=label1)\n",
    "plt.hist(data_label2, bins=30, alpha=0.9, color='green', label=label2)\n",
    "plt.xlabel('true label (red) and predicted label(green)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of frequencies')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "################################################\n",
    "# Your answer to Question 9.2(A) ENDS HERE\n",
    "################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-reserve",
   "metadata": {},
   "source": [
    "**Type your text answers here**\n",
    "\n",
    "**(B)**\n",
    "In the test data set, there is no such a flag which is labeled as black or brown.  However, there are flags labeled as black and brown in the predictions. But generally, the accuracy rate of frequencies for each colour apart from black and brown is above 50%. \n",
    "The reason that the predictions have labeled some flags as black and brown is that the prediction uses the tree based on train data set and train data set has labelled some instances as brown or black.   \n",
    "**(C)**  \n",
    "Accuracy is a correct evaluation metric for the Flags data set. The program uses the tree generated by train data set to predict the label for each flag and includes all labels of flags, which means that all colours are used, in the train data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-convert",
   "metadata": {},
   "source": [
    "### Q9.3 Features (0.75 mark)\n",
    "\n",
    "By looking at the `flags.names` file as well as your helper functions answer the following question. Feel free to implement gain ratio but you can answer this question intuitively too. \n",
    "\n",
    "**(A)** If we were to use `gain ratio` instead of `information gain`, which of the 25 attributes would be most affected? why? **0.75**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "2c6ae29c-cee3-4430-8ed9-57b2ea81b10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue 1\n"
     ]
    }
   ],
   "source": [
    "### CODE 9.3 optional code ###\n",
    "def calculate_GR(root, children, child):\n",
    "    answer = 0\n",
    "    if calc_entopy(child) != 0:\n",
    "        answer = calculate_IG(root, children)/calc_entopy(child)\n",
    "    return answer\n",
    "\n",
    "def find_best_GR(data, features_list):\n",
    "    max = 0\n",
    "    \n",
    "    for a in features_list[0:-1]:\n",
    "        values = df[a]\n",
    "        for i in values:\n",
    "            left, right = find_subtrees(data, features_list, a, i)\n",
    "            if calculate_GR(data,[left, right], left) > max:\n",
    "                max = calculate_GR(data,[left, right], left)\n",
    "                y1 = a\n",
    "                y2 = i\n",
    "            elif calculate_GR(data,[left, right], right) > max:\n",
    "                max = calculate_GR(data,[left, right], right)\n",
    "                y1 = a\n",
    "                y2 = i\n",
    "    return y1,y2\n",
    "\n",
    "feature, value = find_best_GR(train_df.values,train_df.columns)\n",
    "print(feature, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a0f00c-6355-4043-9e06-e2118f298a92",
   "metadata": {},
   "source": [
    "**Type your text answers here**    \n",
    "(blue, 1) is still the most affected attribute based on the result of the program\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7c5ce-8d9b-49b1-91ad-2345f86141be",
   "metadata": {},
   "source": [
    "### Q9.4 Decision tree complexity (1 mark)\n",
    "\n",
    "**(A)** Using the tree you generated in Q8 (name it `little_tree`), find the accuracy of the tree for predicting the labels for the **training set**. Do you notice a difference between train and test accuracy? If so, discuss possible reasons. \n",
    "\n",
    "**(B)** Now change the max_depth of the decision tree to 10 and train another tree (name it `big_tree`). Now use this new tree to predict the labels for test and train sets. Describe and explain any change in the results you notice compared to your tree of depth 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "e8a81b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "### CODE 9.4 (A) ###\n",
    "max_depth = 3\n",
    "little_tree =  decision_tree_algorithm(train_df.values, train_df.columns, max_depth, counter=0)\n",
    "prediction = make_predictions(train_df, little_tree)\n",
    "accuracy = calculate_accuracy(train_df, prediction)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "2d227ce8-0624-4302-b286-9a723a9962e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.43181818181818177\n"
     ]
    }
   ],
   "source": [
    "### CODE 9.4 (B) ###\n",
    "max_depth = 10\n",
    "big_tree = decision_tree_algorithm(train_df.values, train_df.columns, max_depth, counter=0)\n",
    "prediction_train = make_predictions(train_df, big_tree)\n",
    "accuracy_train = calculate_accuracy(train_df, prediction_train)\n",
    "prediction_test = make_predictions(test_df, big_tree)\n",
    "accuracy_test = calculate_accuracy(test_df, prediction_test)\n",
    "print(accuracy_train)\n",
    "print(accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ace3a-8f88-43fb-90b9-375cdbdd57f2",
   "metadata": {},
   "source": [
    "**Type your text answers here**\n",
    "\n",
    "**(A)**  \n",
    "The predictions based on the tree generated by train data set gives the 100% accuracy for the train data set since it uses the tree based on train data set, therefore there is no doubt that It will predict all corret.      \n",
    "**(B)**    \n",
    "The results show that max depth does not affect the accuracy. \n",
    "No matter how many depths this tree has, the nodes are generated based on the attributes of the train data set.\n",
    "The prediction mainly uses features to predict the labels. So the depth of the tree is not matter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cda3b8-38f0-48da-a877-7e395fd06fdb",
   "metadata": {},
   "source": [
    "# Authorship Declaration:\n",
    "\n",
    "   (1) I certify that the program contained in this submission is completely\n",
    "   my own individual work, except where explicitly noted by comments that\n",
    "   provide details otherwise.  I understand that work that has been developed\n",
    "   by another student, or by me in collaboration with other students,\n",
    "   or by non-students as a result of request, solicitation, or payment,\n",
    "   may not be submitted for assessment in this subject.  I understand that\n",
    "   submitting for assessment work developed by or in collaboration with\n",
    "   other students or non-students constitutes Academic Misconduct, and\n",
    "   may be penalized by mark deductions, or by other penalties determined\n",
    "   via the University of Melbourne Academic Honesty Policy, as described\n",
    "   at https://academicintegrity.unimelb.edu.au.\n",
    "\n",
    "   (2) I also certify that I have not provided a copy of this work in either\n",
    "   softcopy or hardcopy or any other form to any other student, and nor will\n",
    "   I do so until after the marks are released. I understand that providing\n",
    "   my work to other students, regardless of my intention or any undertakings\n",
    "   made to me by that other student, is also Academic Misconduct.\n",
    "\n",
    "   (3) I further understand that providing a copy of the assignment\n",
    "   specification to any form of code authoring or assignment tutoring\n",
    "   service, or drawing the attention of others to such services and code\n",
    "   that may have been made available via such a service, may be regarded\n",
    "   as Student General Misconduct (interfering with the teaching activities\n",
    "   of the University and/or inciting others to commit Academic Misconduct).\n",
    "   I understand that an allegation of Student General Misconduct may arise\n",
    "   regardless of whether or not I personally make use of such solutions\n",
    "   or sought benefit from such actions.\n",
    "\n",
    "   <b>Signed by</b>: [Wang Risheng 1053051]\n",
    "   \n",
    "   <b>Dated</b>: [07/08/2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d60b59-0c8e-48c0-a899-54c84d4ee2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
